import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
import aiohttp
import json
from typing import List, Dict, Optional, Tuple
import argparse
import sys

# å°å…¥æˆ‘å€‘çš„å·¥å…·å‡½æ•¸
from utils import (
    config_manager, 
    LoggerSetup, 
    retry_on_failure, 
    get_taiwan_time,
    validate_data_quality,
    get_clean_symbol
)
from loguru import logger

# è¨­å®šæ—¥èªŒ
LoggerSetup.setup_logger("data_collector", config_manager.get("system.log_level", "INFO"))

class MarketDataCollector:
    """å¸‚å ´æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.config = config_manager
        self.data_dir = Path("data/market")
        self.news_dir = Path("data/news")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.news_dir.mkdir(parents=True, exist_ok=True)
        
        self.today = get_taiwan_time().strftime("%Y-%m-%d")
        self.today_news_dir = self.news_dir / self.today
        self.today_news_dir.mkdir(exist_ok=True)
    
    @retry_on_failure(max_retries=3, delay=3.0)
    def fetch_yahoo_data(
        self, 
        symbol: str, 
        period: str = "1y", 
        interval: str = "1d"
    ) -> Optional[pd.DataFrame]:
        """
        å¾ Yahoo Finance ç²å–è‚¡ç¥¨æ•¸æ“š
        """
        try:
            logger.info(f"æ­£åœ¨ç²å– {symbol} çš„ {interval} æ•¸æ“šï¼Œç¯„åœ: {period}")
            ticker = yf.Ticker(symbol)
            data = ticker.history(period=period, interval=interval)
            
            if data.empty:
                logger.warning(f"{symbol} æ²’æœ‰ç²å–åˆ°æ•¸æ“š")
                return None
            
            data['Symbol'] = symbol
            data['Updated'] = get_taiwan_time().isoformat()
            data.reset_index(inplace=True)
            
            if 'Date' in data.columns:
                data.rename(columns={'Date': 'Datetime'}, inplace=True)
            
            required_columns = ['Datetime', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                logger.error(f"{symbol} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            columns = ['Datetime', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'Updated']
            optional_columns = ['Dividends', 'Stock Splits']
            for col in optional_columns:
                if col in data.columns:
                    columns.insert(-1, col)
            
            available_columns = [col for col in columns if col in data.columns]
            data = data[available_columns]
            
            logger.success(f"æˆåŠŸç²å– {symbol} æ•¸æ“šï¼Œå…± {len(data)} ç­†è¨˜éŒ„")
            return data
            
        except Exception as e:
            logger.error(f"ç²å– {symbol} æ•¸æ“šå¤±æ•—: {str(e)}")
            raise
    
    def save_market_data(self, data: pd.DataFrame, symbol: str, data_type: str) -> bool:
        """
        å„²å­˜å¸‚å ´æ•¸æ“šåˆ°ç°¡åŒ–çš„æª”æ¡ˆçµæ§‹
        """
        try:
            clean_symbol = get_clean_symbol(symbol)
            filename = self.data_dir / f"{data_type}_{clean_symbol}.csv"
            
            if filename.exists():
                try:
                    existing_data = pd.read_csv(filename)
                    existing_data['Datetime'] = pd.to_datetime(existing_data['Datetime'])
                    data['Datetime'] = pd.to_datetime(data['Datetime'])
                    combined_data = pd.concat([existing_data, data]).drop_duplicates(
                        subset=['Datetime'], keep='last'
                    ).sort_values('Datetime').reset_index(drop=True)
                    
                    if data_type == "daily":
                        combined_data = combined_data.tail(300)
                    elif data_type == "hourly":
                        cutoff_date = get_taiwan_time() - timedelta(days=14)
                        combined_data = combined_data[combined_data['Datetime'] >= cutoff_date]
                    
                    data_to_save = combined_data
                except Exception as e:
                    logger.warning(f"åˆä½µ {symbol} æ­·å²æ•¸æ“šæ™‚å‡ºéŒ¯ï¼Œä½¿ç”¨æ–°æ•¸æ“š: {str(e)}")
                    data_to_save = data
            else:
                data_to_save = data
            
            data_to_save.to_csv(filename, index=False)
            logger.info(f"æˆåŠŸå„²å­˜ {symbol} çš„ {data_type} æ•¸æ“šåˆ° {filename}")
            return True
            
        except Exception as e:
            logger.error(f"å„²å­˜ {symbol} çš„ {data_type} æ•¸æ“šå¤±æ•—: {str(e)}")
            return False
    
    def collect_taiwan_stocks(self) -> Dict:
        """æ”¶é›†å°è‚¡æ•¸æ“š"""
        logger.info("é–‹å§‹æ”¶é›†å°è‚¡æ•¸æ“š...")
        symbols = self.config.get("markets.taiwan.symbols", {})
        results = {"daily": 0, "hourly": 0, "errors": []}
        
        # æ”¶é›†æ—¥ç·šæ•¸æ“š
        daily_symbols = symbols.get("daily", [])
        logger.info(f"æ”¶é›† {len(daily_symbols)} å€‹å°è‚¡æ—¥ç·šæ•¸æ“š")
        
        for symbol in daily_symbols:
            try:
                data = self.fetch_yahoo_data(symbol, period="300d", interval="1d")
                if data is not None and validate_data_quality(data, symbol):
                    if self.save_market_data(data, symbol, "daily"):
                        results["daily"] += 1
                    else:
                        results["errors"].append(f"{symbol}: å„²å­˜å¤±æ•—")
                else:
                    results["errors"].append(f"{symbol}: æ•¸æ“šå“è³ªä¸åˆæ ¼")
            except Exception as e:
                logger.error(f"è™•ç†å°è‚¡ {symbol} å¤±æ•—: {e}")
                results["errors"].append(f"{symbol}: {str(e)}")
        
        # æ”¶é›†å°æ™‚ç·šæ•¸æ“š
        hourly_symbols = symbols.get("hourly", [])
        logger.info(f"æ”¶é›† {len(hourly_symbols)} å€‹å°è‚¡å°æ™‚ç·šæ•¸æ“š")
        
        for symbol in hourly_symbols:
            try:
                data = self.fetch_yahoo_data(symbol, period="60d", interval="1h")
                if data is not None and validate_data_quality(data, symbol):
                    if self.save_market_data(data, symbol, "hourly"):
                        results["hourly"] += 1
                    else:
                        results["errors"].append(f"{symbol}: å°æ™‚ç·šå„²å­˜å¤±æ•—")
                else:
                    results["errors"].append(f"{symbol}: å°æ™‚ç·šæ•¸æ“šå“è³ªä¸åˆæ ¼")
            except Exception as e:
                logger.error(f"è™•ç†å°è‚¡å°æ™‚ç·š {symbol} å¤±æ•—: {e}")
                results["errors"].append(f"{symbol}: å°æ™‚ç·š {str(e)}")
        
        logger.info(f"å°è‚¡æ•¸æ“šæ”¶é›†å®Œæˆ - æ—¥ç·š: {results['daily']}, å°æ™‚ç·š: {results['hourly']}, éŒ¯èª¤: {len(results['errors'])}")
        return results
    
    def collect_us_stocks(self) -> Dict:
        """æ”¶é›†ç¾è‚¡æ•¸æ“š"""
        logger.info("é–‹å§‹æ”¶é›†ç¾è‚¡æ•¸æ“š...")
        symbols = self.config.get("markets.us.symbols", {})
        results = {"daily": 0, "hourly": 0, "errors": []}
        
        # æ”¶é›†æ—¥ç·šæ•¸æ“š
        daily_symbols = symbols.get("daily", [])
        logger.info(f"æ”¶é›† {len(daily_symbols)} å€‹ç¾è‚¡æ—¥ç·šæ•¸æ“š")
        
        for symbol in daily_symbols:
            try:
                data = self.fetch_yahoo_data(symbol, period="300d", interval="1d")
                if data is not None and validate_data_quality(data, symbol):
                    if self.save_market_data(data, symbol, "daily"):
                        results["daily"] += 1
                    else:
                        results["errors"].append(f"{symbol}: å„²å­˜å¤±æ•—")
                else:
                    results["errors"].append(f"{symbol}: æ•¸æ“šå“è³ªä¸åˆæ ¼")
            except Exception as e:
                logger.error(f"è™•ç†ç¾è‚¡ {symbol} å¤±æ•—: {e}")
                results["errors"].append(f"{symbol}: {str(e)}")
        
        # æ”¶é›†å°æ™‚ç·šæ•¸æ“š
        hourly_symbols = symbols.get("hourly", [])
        logger.info(f"æ”¶é›† {len(hourly_symbols)} å€‹ç¾è‚¡å°æ™‚ç·šæ•¸æ“š")
        
        for symbol in hourly_symbols:
            try:
                data = self.fetch_yahoo_data(symbol, period="60d", interval="1h")
                if data is not None and validate_data_quality(data, symbol):
                    if self.save_market_data(data, symbol, "hourly"):
                        results["hourly"] += 1
                    else:
                        results["errors"].append(f"{symbol}: å°æ™‚ç·šå„²å­˜å¤±æ•—")
                else:
                    results["errors"].append(f"{symbol}: å°æ™‚ç·šæ•¸æ“šå“è³ªä¸åˆæ ¼")
            except Exception as e:
                logger.error(f"è™•ç†ç¾è‚¡å°æ™‚ç·š {symbol} å¤±æ•—: {e}")
                results["errors"].append(f"{symbol}: å°æ™‚ç·š {str(e)}")
        
        logger.info(f"ç¾è‚¡æ•¸æ“šæ”¶é›†å®Œæˆ - æ—¥ç·š: {results['daily']}, å°æ™‚ç·š: {results['hourly']}, éŒ¯èª¤: {len(results['errors'])}")
        return results
    
    def collect_crypto_data(self) -> Dict:
        """æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š"""
        logger.info("é–‹å§‹æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š...")
        symbols = self.config.get("markets.crypto.symbols", {})
        results = {"daily": 0, "hourly": 0, "errors": []}
        
        # æ”¶é›†æ—¥ç·šæ•¸æ“š
        daily_symbols = symbols.get("daily", [])
        logger.info(f"æ”¶é›† {len(daily_symbols)} å€‹åŠ å¯†è²¨å¹£æ—¥ç·šæ•¸æ“š")
        
        for symbol in daily_symbols:
            try:
                data = self.fetch_yahoo_data(symbol, period="300d", interval="1d")
                if data is not None and validate_data_quality(data, symbol):
                    if self.save_market_data(data, symbol, "daily"):
                        results["daily"] += 1
                    else:
                        results["errors"].append(f"{symbol}: å„²å­˜å¤±æ•—")
                else:
                    results["errors"].append(f"{symbol}: æ•¸æ“šå“è³ªä¸åˆæ ¼")
            except Exception as e:
                logger.error(f"è™•ç†åŠ å¯†è²¨å¹£ {symbol} å¤±æ•—: {e}")
                results["errors"].append(f"{symbol}: {str(e)}")
        
        # æ”¶é›†å°æ™‚ç·šæ•¸æ“š
        hourly_symbols = symbols.get("hourly", [])
        logger.info(f"æ”¶é›† {len(hourly_symbols)} å€‹åŠ å¯†è²¨å¹£å°æ™‚ç·šæ•¸æ“š")
        
        for symbol in hourly_symbols:
            try:
                data = self.fetch_yahoo_data(symbol, period="60d", interval="1h")
                if data is not None and validate_data_quality(data, symbol):
                    if self.save_market_data(data, symbol, "hourly"):
                        results["hourly"] += 1
                    else:
                        results["errors"].append(f"{symbol}: å°æ™‚ç·šå„²å­˜å¤±æ•—")
                else:
                    results["errors"].append(f"{symbol}: å°æ™‚ç·šæ•¸æ“šå“è³ªä¸åˆæ ¼")
            except Exception as e:
                logger.error(f"è™•ç†åŠ å¯†è²¨å¹£å°æ™‚ç·š {symbol} å¤±æ•—: {e}")
                results["errors"].append(f"{symbol}: å°æ™‚ç·š {str(e)}")
        
        logger.info(f"åŠ å¯†è²¨å¹£æ•¸æ“šæ”¶é›†å®Œæˆ - æ—¥ç·š: {results['daily']}, å°æ™‚ç·š: {results['hourly']}, éŒ¯èª¤: {len(results['errors'])}")
        return results
    
    async def fetch_news(self, url: str, category: str) -> List[Dict]:
        """ç•°æ­¥æŠ“å–æ–°è RSS"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                async with session.get(url, timeout=30) as response:
                    text = await response.text()
                    feed = feedparser.parse(text)
                    
                    news_items = []
                    max_items = self.config.get("data_collection.news_limit", 5)
                    
                    for entry in feed.entries[:max_items]:
                        news_items.append({
                            'title': entry.get('title', ''),
                            'link': entry.get('link', ''),
                            'published': entry.get('published', ''),
                            'summary': entry.get('summary', ''),
                            'category': category,
                            'collected_at': get_taiwan_time().isoformat()
                        })
                    
                    logger.info(f"æˆåŠŸæŠ“å– {len(news_items)} å‰‡æ–°è (é¡åˆ¥: {category})")
                    return news_items
                    
        except Exception as e:
            logger.error(f"æŠ“å–æ–°è RSS å¤±æ•— ({url}): {str(e)}")
            return []

class NewsCollector:
    """æ–°èæ”¶é›†å™¨"""
    
    def __init__(self, market_collector: MarketDataCollector):
        self.market_collector = market_collector
        self.config = market_collector.config
    
    async def collect_news_for_market(self, market: str, news_sources: List[str]) -> int:
        """é€šç”¨æ–°èæ”¶é›†æ–¹æ³•"""
        news_count = 0
        
        for url in news_sources:
            try:
                # ç°¡åŒ–åˆ†é¡è™•ç†ï¼Œæ¯å€‹URLæ”¶é›†ä¸€æ¬¡æ–°è
                news_items = await self.market_collector.fetch_news(url, market)
                
                for i, item in enumerate(news_items):
                    filename = self.market_collector.today_news_dir / f"news_{market}_{news_count:03d}.json"
                    
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(item, f, ensure_ascii=False, indent=2)
                    
                    news_count += 1
                    logger.debug(f"ä¿å­˜ {market} æ–°è: {filename}")
                    
            except Exception as e:
                logger.error(f"è™•ç† {market} æ–°è {url} å¤±æ•—: {str(e)}")
        
        logger.info(f"å…±æ”¶é›† {news_count} ç¯‡ {market} æ–°è")
        return news_count
    
    async def collect_taiwan_news(self) -> int:
        """æ”¶é›†å°è‚¡æ–°è"""
        news_sources = self.config.get("markets.taiwan.news_sources", [])
        return await self.collect_news_for_market("taiwan", news_sources)
    
    async def collect_us_news(self) -> int:
        """æ”¶é›†ç¾è‚¡æ–°è"""
        news_sources = self.config.get("markets.us.news_sources", [])
        return await self.collect_news_for_market("us", news_sources)

class DataCollector:
    """æ•´é«”æ•¸æ“šæ”¶é›†å”èª¿å™¨"""
    
    def __init__(self):
        self.market_collector = MarketDataCollector()
        self.news_collector = NewsCollector(self.market_collector)
    
    async def collect_all_data(self, market: str = "all") -> Dict:
        """æ”¶é›†æ‰€æœ‰æŒ‡å®šå¸‚å ´çš„æ•¸æ“š"""
        results = {
            'status': 'success',
            'market_data': {'taiwan': {}, 'us': {}, 'crypto': {}},
            'news_data': {'taiwan': 0, 'us': 0},
            'summary': {'total_market_files': 0, 'total_news': 0, 'total_errors': 0},
            'error_message': None,
            'errors': []
        }
        
        start_time = get_taiwan_time()
        total_files = 0
        total_news = 0
        all_errors = []
        
        try:
            if market in ["all", "taiwan"]:
                logger.info("æ”¶é›†å°è‚¡æ•¸æ“š...")
                taiwan_data = self.market_collector.collect_taiwan_stocks()
                results['market_data']['taiwan'] = taiwan_data
                total_files += taiwan_data['daily'] + taiwan_data['hourly']
                all_errors.extend(taiwan_data.get('errors', []))
                
                logger.info("æ”¶é›†å°è‚¡æ–°è...")
                taiwan_news_count = await self.news_collector.collect_taiwan_news()
                results['news_data']['taiwan'] = taiwan_news_count
                total_news += taiwan_news_count
            
            if market in ["all", "us"]:
                logger.info("æ”¶é›†ç¾è‚¡æ•¸æ“š...")
                us_data = self.market_collector.collect_us_stocks()
                results['market_data']['us'] = us_data
                total_files += us_data['daily'] + us_data['hourly']
                all_errors.extend(us_data.get('errors', []))
                
                logger.info("æ”¶é›†ç¾è‚¡æ–°è...")
                us_news_count = await self.news_collector.collect_us_news()
                results['news_data']['us'] = us_news_count
                total_news += us_news_count
            
            if market in ["all", "crypto"]:
                logger.info("æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š...")
                crypto_data = self.market_collector.collect_crypto_data()
                results['market_data']['crypto'] = crypto_data
                total_files += crypto_data['daily'] + crypto_data['hourly']
                all_errors.extend(crypto_data.get('errors', []))
            
            end_time = get_taiwan_time()
            duration = (end_time - start_time).total_seconds()
            
            results['duration_seconds'] = duration
            results['completion_time'] = end_time.isoformat()
            results['summary']['total_market_files'] = total_files
            results['summary']['total_news'] = total_news
            results['summary']['total_errors'] = len(all_errors)
            results['errors'] = all_errors
            
            # å¦‚æœæœ‰éŒ¯èª¤ä½†ä»æœ‰æˆåŠŸæ”¶é›†çš„æ•¸æ“šï¼Œæ¨™è¨˜ç‚ºéƒ¨åˆ†æˆåŠŸ
            if all_errors and total_files > 0:
                results['status'] = 'partial_success'
                logger.warning(f"æ•¸æ“šæ”¶é›†éƒ¨åˆ†æˆåŠŸï¼Œå…± {len(all_errors)} å€‹éŒ¯èª¤")
            elif all_errors and total_files == 0:
                results['status'] = 'error'
                results['error_message'] = f"æ•¸æ“šæ”¶é›†å¤±æ•—ï¼Œå…± {len(all_errors)} å€‹éŒ¯èª¤"
            
            logger.success(f"æ•¸æ“šæ”¶é›†å®Œæˆï¼Œè€—æ™‚ {duration:.2f} ç§’ï¼Œè™•ç† {total_files} å€‹å¸‚å ´æ•¸æ“šæª”æ¡ˆï¼Œ{total_news} å‰‡æ–°è")
            
        except Exception as e:
            logger.error(f"æ•¸æ“šæ”¶é›†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            results['status'] = 'error'
            results['error_message'] = str(e)
        
        return results
    
    def cleanup_old_data(self, retention_days: int = 30):
        """æ¸…ç†èˆŠæ•¸æ“š"""
        logger.info(f"é–‹å§‹æ¸…ç† {retention_days} å¤©å‰çš„èˆŠæ•¸æ“š")
        
        cutoff_date = get_taiwan_time() - timedelta(days=retention_days)
        cutoff_str = cutoff_date.strftime("%Y-%m-%d")
        
        news_dir = Path("data/news")
        if news_dir.exists():
            cleaned_dirs = 0
            for date_dir in news_dir.iterdir():
                if date_dir.is_dir() and date_dir.name < cutoff_str:
                    import shutil
                    shutil.rmtree(date_dir)
                    cleaned_dirs += 1
                    logger.info(f"å·²åˆªé™¤èˆŠæ–°èç›®éŒ„: {date_dir}")
            
            if cleaned_dirs == 0:
                logger.info("æ²’æœ‰éœ€è¦æ¸…ç†çš„èˆŠæ–°èæ•¸æ“š")
            else:
                logger.success(f"å·²æ¸…ç† {cleaned_dirs} å€‹èˆŠæ–°èç›®éŒ„")
    
    def get_data_status(self) -> Dict:
        """ç²å–æ•¸æ“šç‹€æ…‹çµ±è¨ˆ"""
        data_dir = Path("data/market")
        news_dir = Path("data/news")
        
        status = {
            'market_files': {
                'daily': 0,
                'hourly': 0,
                'total': 0
            },
            'news_dirs': 0,
            'last_updated': None,
            'data_health': 'unknown'
        }
        
        if data_dir.exists():
            for file in data_dir.glob("*.csv"):
                if file.name.startswith("daily_"):
                    status['market_files']['daily'] += 1
                elif file.name.startswith("hourly_"):
                    status['market_files']['hourly'] += 1
                status['market_files']['total'] += 1
        
        if news_dir.exists():
            status['news_dirs'] = len([d for d in news_dir.iterdir() if d.is_dir()])
        
        if data_dir.exists():
            csv_files = list(data_dir.glob("*.csv"))
            if csv_files:
                latest_file = max(csv_files, key=lambda f: f.stat().st_mtime)
                status['last_updated'] = datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat()
                
                # æª¢æŸ¥æ•¸æ“šå¥åº·ç‹€æ…‹
                now = get_taiwan_time()
                last_update = datetime.fromtimestamp(latest_file.stat().st_mtime)
                hours_since_update = (now - last_update).total_seconds() / 3600
                
                if hours_since_update < 6:
                    status['data_health'] = 'healthy'
                elif hours_since_update < 24:
                    status['data_health'] = 'warning'
                else:
                    status['data_health'] = 'stale'
        
        return status

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="æ•¸æ“šæ”¶é›†æ™ºå£«")
    parser.add_argument(
        "--market", 
        choices=["all", "taiwan", "us", "crypto"],
        default="all",
        help="æŒ‡å®šæ”¶é›†çš„å¸‚å ´æ•¸æ“š"
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="åŸ·è¡ŒèˆŠæ•¸æ“šæ¸…ç†"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="æ¸¬è©¦æ¨¡å¼ï¼ˆåªæ”¶é›†å°‘é‡æ•¸æ“šï¼‰"
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="é¡¯ç¤ºæ•¸æ“šç‹€æ…‹"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="å•Ÿç”¨èª¿è©¦æ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    if args.debug:
        logger.remove()
        LoggerSetup.setup_logger("data_collector", "DEBUG")
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«å•Ÿå‹• ===")
    
    collector = DataCollector()
    
    if args.status:
        status = collector.get_data_status()
        logger.info("æ•¸æ“šç‹€æ…‹çµ±è¨ˆ:")
        logger.info(f"  å¸‚å ´æ•¸æ“šæª”æ¡ˆ: {status['market_files']['total']} å€‹ (æ—¥ç·š: {status['market_files']['daily']}, å°æ™‚ç·š: {status['market_files']['hourly']})")
        logger.info(f"  æ–°èç›®éŒ„: {status['news_dirs']} å€‹")
        logger.info(f"  æ•¸æ“šå¥åº·ç‹€æ…‹: {status['data_health']}")
        if status['last_updated']:
            logger.info(f"  æœ€å¾Œæ›´æ–°: {status['last_updated']}")
        return
    
    if args.cleanup:
        retention_days = config_manager.get("data_collection.data_retention_days", 30)
        collector.cleanup_old_data(retention_days)
    
    if args.test:
        logger.info("æ¸¬è©¦æ¨¡å¼ï¼šåªæ”¶é›†å–®ä¸€è‚¡ç¥¨æ•¸æ“š")
        test_symbol = "0050.TW" if args.market in ["taiwan", "all"] else "QQQ"
        try:
            data = collector.market_collector.fetch_yahoo_data(test_symbol, period="5d", interval="1d")
            if data is not None and len(data) > 0:
                logger.success(f"æ¸¬è©¦æˆåŠŸï¼ç²å–åˆ° {len(data)} ç­† {test_symbol} æ•¸æ“š")
            else:
                logger.error("æ¸¬è©¦å¤±æ•—ï¼æ²’æœ‰ç²å–åˆ°æ•¸æ“š")
                sys.exit(1)
        except Exception as e:
            logger.error(f"æ¸¬è©¦å¤±æ•—: {e}")
            sys.exit(1)
    else:
        results = asyncio.run(collector.collect_all_data(args.market))
        
        if results['status'] == 'success':
            logger.success("âœ… æ•¸æ“šæ”¶é›†ä»»å‹™å®Œæˆï¼")
        elif results['status'] == 'partial_success':
            logger.warning("âš ï¸ æ•¸æ“šæ”¶é›†éƒ¨åˆ†å®Œæˆï¼Œå­˜åœ¨ä¸€äº›éŒ¯èª¤")
            if args.debug and results.get('errors'):
                for error in results['errors'][:5]:  # åªé¡¯ç¤ºå‰5å€‹éŒ¯èª¤
                    logger.warning(f"  - {error}")
        else:
            logger.error("âŒ æ•¸æ“šæ”¶é›†ä»»å‹™å¤±æ•—ï¼")
            if results.get('error_message'):
                logger.error(f"éŒ¯èª¤è¨Šæ¯: {results['error_message']}")
            sys.exit(1)
        
        if 'summary' in results:
            summary = results['summary']
            logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ: å¸‚å ´æª”æ¡ˆ {summary['total_market_files']} å€‹, æ–°è {summary['total_news']} å‰‡")
            if summary['total_errors'] > 0:
                logger.warning(f"âš ï¸ å…± {summary['total_errors']} å€‹éŒ¯èª¤")
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«çµæŸ ===")

if __name__ == "__main__":
    main()
