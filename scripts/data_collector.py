# scripts/data_collector.py
import os
import json
import logging
from datetime import datetime, timedelta

import yfinance as yf
import pandas as pd

# === å»ºç«‹ logger ===
def setup_logger(name, log_file, level=logging.INFO):
    logger = logging.getLogger(name)
    logger.setLevel(level)
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

    fh = logging.FileHandler(log_file, encoding="utf-8")
    fh.setFormatter(formatter)
    fh.setLevel(level)

    eh = logging.FileHandler("logs/error.log", encoding="utf-8")
    eh.setFormatter(formatter)
    eh.setLevel(logging.ERROR)

    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    ch.setLevel(level)

    if not logger.handlers:
        logger.addHandler(fh)
        logger.addHandler(eh)
        logger.addHandler(ch)

    return logger

logger = setup_logger("data_collector", "logs/data_collector.log")

# === å·¥å…·å‡½å¼ ===
def load_config(config_path="config.json"):
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"è¼‰å…¥ config.json å¤±æ•—: {e}")
        return {}

def fetch_data(symbol, interval, lookback_days):
    """ä¸‹è¼‰ Yahoo Finance è³‡æ–™ï¼Œé‡è©¦ 3 æ¬¡"""
    for attempt in range(3):
        try:
            start_date = (datetime.now() - timedelta(days=lookback_days)).strftime("%Y-%m-%d")
            df = yf.download(symbol, start=start_date, interval=interval, progress=False)

            if df is None or df.empty:
                raise ValueError("ä¸‹è¼‰çµæœç‚ºç©º")

            df.reset_index(inplace=True)
            if "Datetime" in df.columns:
                df.rename(columns={"Datetime": "Date"}, inplace=True)

            # å®Œå…¨ä¿ç•™ Yahoo åŸå§‹æ—¥æœŸæ ¼å¼ï¼Œä¸è½‰æ›ã€ä¸æ”¹æ ¼å¼
            # df["Date"] = pd.to_datetime(df["Date"])  # å¯é¸ï¼Œç”¨æ–¼ç¢ºä¿ datetime type

            return df
        except Exception as e:
            logger.warning(f"âš ï¸ ç¬¬ {attempt+1}/3 æ¬¡æŠ“å– {symbol} ({interval}) å¤±æ•—: {e}")
    logger.error(f"âŒ æŠ“å– {symbol} ({interval}) æœ€çµ‚å¤±æ•—")
    return None

def save_data(df, filepath, max_rows):
    """ä¿å­˜ CSVï¼Œåªä¿ç•™æŒ‡å®šç­†æ•¸"""
    try:
        df = df.tail(max_rows)
        df.to_csv(filepath, index=False, encoding="utf-8")
        logger.info(f"âœ… å·²æ›´æ–° {filepath} ({len(df)} ç­†) | æœ€å¾Œä¸€ç­†: {df.iloc[-1]['Date']} Close={df.iloc[-1]['Close']}")
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜ {filepath} å¤±æ•—: {e}")

# === ä¸»ç¨‹å¼ ===
def main():
    config = load_config()
    if not config or "symbols" not in config:
        logger.error("âŒ config.json ç¼ºå°‘ symbols è¨­å®š")
        return

    os.makedirs("data", exist_ok=True)
    os.makedirs("logs", exist_ok=True)

    for symbol in config["symbols"]:
        logger.info(f"ğŸ“¥ è™•ç† {symbol}")

        # æ—¥ç·šï¼Œä¿ç•™ 300 å¤©
        daily = fetch_data(symbol, "1d", 365)
        if daily is not None:
            save_data(daily, f"data/daily_{symbol}.csv", 300)

        # å°æ™‚ç·šï¼Œä¿ç•™æœ€è¿‘ 14 å¤©è³‡æ–™
        hourly = fetch_data(symbol, "60m", 30)
        if hourly is not None:
            save_data(hourly, f"data/hourly_{symbol}.csv", 14 * 7)  # ä¸€å¤©ç´„7ç­†äº¤æ˜“å°æ™‚

if __name__ == "__main__":
    logger.info("ğŸš€ Data Collector é–‹å§‹åŸ·è¡Œ")
    main()
    logger.info("ğŸ Data Collector çµæŸ")