import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
import aiohttp
from typing import List, Dict, Optional, Tuple
import argparse
import sys

# å°å…¥æˆ‘å€‘çš„å·¥å…·å‡½æ•¸
from utils import (
    config_manager, 
    LoggerSetup, 
    retry_on_failure, 
    get_taiwan_time,
    validate_data_quality
)
from loguru import logger

# è¨­å®šæ—¥èªŒ
LoggerSetup.setup_logger("data_collector", config_manager.get("system.log_level", "INFO"))

class MarketDataCollector:
    """å¸‚å ´æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.config = config_manager
        # ç°¡åŒ–æ•¸æ“šç›®éŒ„çµæ§‹
        self.data_dir = Path("data/market")
        self.news_dir = Path("data/news")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.news_dir.mkdir(parents=True, exist_ok=True)
        
        # å‰µå»ºä»Šå¤©çš„æ–°èç›®éŒ„ï¼ˆæ–°èé‚„æ˜¯æŒ‰æ—¥æœŸåˆ†çµ„ï¼‰
        self.today = get_taiwan_time().strftime("%Y-%m-%d")
        self.today_news_dir = self.news_dir / self.today
        self.today_news_dir.mkdir(exist_ok=True)
    
    @retry_on_failure(max_retries=3, delay=3.0)
    def fetch_yahoo_data(
        self, 
        symbol: str, 
        period: str = "1y", 
        interval: str = "1d"
    ) -> Optional[pd.DataFrame]:
        """
        å¾ Yahoo Finance ç²å–è‚¡ç¥¨æ•¸æ“š
        
        Args:
            symbol: è‚¡ç¥¨ä»£è™Ÿ
            period: æ™‚é–“ç¯„åœ (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)
            interval: æ•¸æ“šé–“éš” (1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo)
        """
        try:
            logger.info(f"æ­£åœ¨ç²å– {symbol} çš„ {interval} æ•¸æ“šï¼Œç¯„åœ: {period}")
            
            ticker = yf.Ticker(symbol)
            data = ticker.history(period=period, interval=interval)
            
            if data.empty:
                logger.warning(f"{symbol} æ²’æœ‰ç²å–åˆ°æ•¸æ“š")
                return None
            
            # æ·»åŠ è‚¡ç¥¨ä»£è™Ÿå’Œæ›´æ–°æ™‚é–“æ¬„ä½
            data['Symbol'] = symbol
            data['Updated'] = get_taiwan_time().isoformat()
            
            # é‡è¨­ç´¢å¼•ï¼Œå°‡æ—¥æœŸä½œç‚ºæ¬„ä½
            data.reset_index(inplace=True)
            
            # æª¢æŸ¥æ™‚é–“æ¬„ä½åç¨±ä¸¦çµ±ä¸€ç‚º 'Datetime'
            if 'Date' in data.columns:
                data.rename(columns={'Date': 'Datetime'}, inplace=True)
            
            # ç¢ºä¿åŸºæœ¬æ¬„ä½å­˜åœ¨
            required_columns = ['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                logger.error(f"{symbol} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            # é‡æ–°æ’åºæ¬„ä½ï¼Œè®“æ™‚é–“æ¬„ä½åœ¨å‰é¢
            columns = ['Datetime', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'Updated']
            
            # æ·»åŠ å¯é¸æ¬„ä½ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            optional_columns = ['Dividends', 'Stock Splits']
            for col in optional_columns:
                if col in data.columns:
                    columns.insert(-1, col)
            
            # åªé¸æ“‡å­˜åœ¨çš„æ¬„ä½
            available_columns = [col for col in columns if col in data.columns]
            data = data[available_columns]
            
            logger.success(f"æˆåŠŸç²å– {symbol} æ•¸æ“šï¼Œå…± {len(data)} ç­†è¨˜éŒ„")
            return data
            
        except Exception as e:
            logger.error(f"ç²å– {symbol} æ•¸æ“šå¤±æ•—: {str(e)}")
            raise
    
    def save_market_data(self, data: pd.DataFrame, symbol: str, data_type: str) -> bool:
        """
        å„²å­˜å¸‚å ´æ•¸æ“šåˆ°ç°¡åŒ–çš„æª”æ¡ˆçµæ§‹
        
        Args:
            data: DataFrame
            symbol: è‚¡ç¥¨ä»£è™Ÿ
            data_type: "daily" æˆ– "hourly"
        """
        try:
            # æ¸…ç†ç¬¦è™Ÿåç¨±ä½œç‚ºæª”æ¡ˆå
            clean_symbol = symbol.replace('^', '').replace('.', '_').replace('=', '_').replace('-', '_')
            filename = self.data_dir / f"{data_type}_{clean_symbol}.csv"
            
            # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if filename.exists():
                try:
                    existing_data = pd.read_csv(filename)
                    
                    # ç¢ºä¿å…©å€‹ DataFrame éƒ½æœ‰ Datetime æ¬„ä½ä¸”æ ¼å¼ä¸€è‡´
                    if 'Datetime' in existing_data.columns:
                        existing_data['Datetime'] = pd.to_datetime(existing_data['Datetime'])
                    
                    if 'Datetime' in data.columns:
                        data['Datetime'] = pd.to_datetime(data['Datetime'])
                    else:
                        logger.error(f"{symbol} æ–°æ•¸æ“šç¼ºå°‘ Datetime æ¬„ä½")
                        return False
                    
                    # åˆä½µæ–°èˆŠæ•¸æ“šï¼Œå»é‡ä¸¦æ’åº
                    combined_data = pd.concat([existing_data, data]).drop_duplicates(
                        subset=['Datetime'], keep='last'
                    ).sort_values('Datetime').reset_index(drop=True)
                    
                    # æ ¹æ“šæ•¸æ“šé¡å‹ä¿ç•™é©ç•¶çš„è¨˜éŒ„æ•¸é‡
                    if data_type == "daily":
                        # ä¿ç•™æœ€è¿‘300å¤©
                        combined_data = combined_data.tail(300)
                    elif data_type == "hourly":
                        # ä¿ç•™æœ€è¿‘14å¤©çš„æ•¸æ“š
                        cutoff_date = get_taiwan_time() - timedelta(days=14)
                        combined_data = combined_data[combined_data['Datetime'] >= cutoff_date]
                    
                    data_to_save = combined_data
                    
                except Exception as e:
                    logger.warning(f"åˆä½µ {symbol} æ­·å²æ•¸æ“šæ™‚å‡ºéŒ¯ï¼Œä½¿ç”¨æ–°æ•¸æ“š: {str(e)}")
                    data_to_save = data
            else:
                data_to_save = data
            
            # ä¿å­˜æ•¸æ“š
            data_to_save.to_csv(filename, index=False)
            logger.info(f"æˆåŠŸå„²å­˜ {symbol} çš„ {data_type} æ•¸æ“šåˆ° {filename}")
            return True
            
        except Exception as e:
            logger.error(f"å„²å­˜ {symbol} çš„ {data_type} æ•¸æ“šå¤±æ•—: {str(e)}")
            return False
    
    async def fetch_news(self, url: str, category: str) -> List[Dict]:
        """ç•°æ­¥æŠ“å–æ–°è RSS"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    text = await response.text()
                    feed = feedparser.parse(text)
                    
                    news_items = []
                    for entry in feed.entries[:self.config.get("data_collection.news_limit", 3)]:
                        news_items.append({
                            'title': entry.get('title', ''),
                            'link': entry.get('link', ''),
                            'published': entry.get('published', ''),
                            'summary': entry.get('summary', ''),
                            'category': category
                        })
                    
                    return news_items
                    
        except Exception as e:
            logger.error(f"æŠ“å–æ–°è RSS å¤±æ•— ({url}): {str(e)}")
            return []
    
    def collect_us_stocks(self) -> Dict:
        """æ”¶é›†ç¾è‚¡æ•¸æ“š"""
        results = {'daily': 0, 'hourly': 0}
        symbols = self.config.get("markets.us.symbols.daily", [])
        for symbol in symbols:
            data = self.fetch_yahoo_data(symbol, period="1y", interval="1d")
            if data is not None and validate_data_quality(data, symbol):
                if self.save_market_data(data, symbol, "daily"):
                    results['daily'] += 1
        
        hourly_symbols = self.config.get("markets.us.symbols.hourly", [])
        for symbol in hourly_symbols:
            data = self.fetch_yahoo_data(symbol, period="14d", interval="1h")
            if data is not None and validate_data_quality(data, symbol):
                if self.save_market_data(data, symbol, "hourly"):
                    results['hourly'] += 1
        
        return results
    
    def collect_taiwan_stocks(self) -> Dict:
        """æ”¶é›†å°è‚¡æ•¸æ“š"""
        results = {'daily': 0, 'hourly': 0}
        symbols = self.config.get("markets.taiwan.symbols.daily", [])
        for symbol in symbols:
            data = self.fetch_yahoo_data(symbol, period="1y", interval="1d")
            if data is not None and validate_data_quality(data, symbol):
                if self.save_market_data(data, symbol, "daily"):
                    results['daily'] += 1
        
        hourly_symbols = self.config.get("markets.taiwan.symbols.hourly", [])
        for symbol in hourly_symbols:
            data = self.fetch_yahoo_data(symbol, period="14d", interval="1h")
            if data is not None and validate_data_quality(data, symbol):
                if self.save_market_data(data, symbol, "hourly"):
                    results['hourly'] += 1
        
        return results
    
    def collect_crypto_data(self) -> Dict:
        """æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š"""
        results = {'daily': 0, 'hourly': 0}
        symbols = self.config.get("markets.crypto.symbols.daily", [])
        for symbol in symbols:
            data = self.fetch_yahoo_data(symbol, period="1y", interval="1d")
            if data is not None and validate_data_quality(data, symbol):
                if self.save_market_data(data, symbol, "daily"):
                    results['daily'] += 1
        
        return results

class NewsCollector:
    """æ–°èæ”¶é›†å™¨"""
    
    def __init__(self, market_collector: MarketDataCollector):
        self.market_collector = market_collector
        self.config = market_collector.config
    
    async def collect_taiwan_news(self) -> int:
        """æ”¶é›†å°è‚¡æ–°è"""
        news_urls = self.config.get("markets.taiwan.news_sources", [])
        categories = self.config.get("news_categories", ["ç¶“æ¿Ÿ", "åŠå°é«”"])
        news_count = 0
        
        for url in news_urls:
            for category in categories:
                news_items = await self.market_collector.fetch_news(url, category)
                for item in news_items:
                    filename = self.market_collector.today_news_dir / f"news_taiwan_{news_count}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(item, f, ensure_ascii=False, indent=2)
                    news_count += 1
        
        return news_count
    
    async def collect_us_news(self) -> int:
        """æ”¶é›†ç¾è‚¡æ–°è"""
        news_urls = self.config.get("markets.us.news_sources", [])
        categories = self.config.get("news_categories", ["ç¶“æ¿Ÿ", "åŠå°é«”"])
        news_count = 0
        
        for url in news_urls:
            for category in categories:
                news_items = await self.market_collector.fetch_news(url, category)
                for item in news_items:
                    filename = self.market_collector.today_news_dir / f"news_us_{news_count}.json"
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(item, f, ensure_ascii=False, indent=2)
                    news_count += 1
        
        return news_count

class DataCollector:
    """æ•´é«”æ•¸æ“šæ”¶é›†å”èª¿å™¨"""
    
    def __init__(self):
        self.market_collector = MarketDataCollector()
        self.news_collector = NewsCollector(self.market_collector)
    
    async def collect_all_data(self, market: str = "all") -> Dict:
        """æ”¶é›†æ‰€æœ‰æŒ‡å®šå¸‚å ´çš„æ•¸æ“š"""
        results = {
            'status': 'success',
            'market_data': {'taiwan': {}, 'us': {}, 'crypto': {}},
            'news_data': {'taiwan': 0, 'us': 0},
            'summary': {'total_market_files': 0},
            'error_message': None
        }
        
        start_time = get_taiwan_time()
        total_files = 0
        
        try:
            if market in ["all", "taiwan"]:
                logger.info("æ”¶é›†å°è‚¡æ•¸æ“š...")
                taiwan_data = self.market_collector.collect_taiwan_stocks()
                results['market_data']['taiwan'] = taiwan_data
                total_files += taiwan_data['daily'] + taiwan_data['hourly']
                
                logger.info("æ”¶é›†å°è‚¡æ–°è...")
                taiwan_news_count = await self.news_collector.collect_taiwan_news()
                results['news_data']['taiwan'] = taiwan_news_count
            
            if market in ["all", "us"]:
                logger.info("æ”¶é›†ç¾è‚¡æ•¸æ“š...")
                us_data = self.market_collector.collect_us_stocks()
                results['market_data']['us'] = us_data
                total_files += us_data['daily'] + us_data['hourly']
                
                logger.info("æ”¶é›†ç¾è‚¡æ–°è...")
                us_news_count = await self.news_collector.collect_us_news()
                results['news_data']['us'] = us_news_count
            
            if market in ["all", "crypto"]:
                logger.info("æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š...")
                crypto_data = self.market_collector.collect_crypto_data()
                results['market_data']['crypto'] = crypto_data
                total_files += crypto_data['daily'] + crypto_data['hourly']
            
            end_time = get_taiwan_time()
            duration = (end_time - start_time).total_seconds()
            
            results['duration_seconds'] = duration
            results['completion_time'] = end_time.isoformat()
            results['summary']['total_market_files'] = total_files
            
            logger.success(f"æ•¸æ“šæ”¶é›†å®Œæˆï¼Œè€—æ™‚ {duration:.2f} ç§’ï¼Œå…±è™•ç† {total_files} å€‹å¸‚å ´æ•¸æ“šæª”æ¡ˆ")
            
        except Exception as e:
            logger.error(f"æ•¸æ“šæ”¶é›†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            results['status'] = 'error'
            results['error_message'] = str(e)
        
        return results
    
    def cleanup_old_data(self, retention_days: int = 30):
        """æ¸…ç†èˆŠæ•¸æ“š"""
        logger.info(f"é–‹å§‹æ¸…ç† {retention_days} å¤©å‰çš„èˆŠæ•¸æ“š")
        
        cutoff_date = get_taiwan_time() - timedelta(days=retention_days)
        cutoff_str = cutoff_date.strftime("%Y-%m-%d")
        
        # åªæ¸…ç†æ–°èæ•¸æ“šï¼ˆæŒ‰æ—¥æœŸåˆ†çµ„ï¼‰
        news_dir = Path("data/news")
        if news_dir.exists():
            cleaned_dirs = 0
            for date_dir in news_dir.iterdir():
                if date_dir.is_dir() and date_dir.name < cutoff_str:
                    import shutil
                    shutil.rmtree(date_dir)
                    cleaned_dirs += 1
                    logger.info(f"å·²åˆªé™¤èˆŠæ–°èç›®éŒ„: {date_dir}")
            
            if cleaned_dirs == 0:
                logger.info("æ²’æœ‰éœ€è¦æ¸…ç†çš„èˆŠæ–°èæ•¸æ“š")
            else:
                logger.success(f"å·²æ¸…ç† {cleaned_dirs} å€‹èˆŠæ–°èç›®éŒ„")
    
    def get_data_status(self) -> Dict:
        """ç²å–æ•¸æ“šç‹€æ…‹çµ±è¨ˆ"""
        data_dir = Path("data/market")
        news_dir = Path("data/news")
        
        status = {
            'market_files': {
                'daily': 0,
                'hourly': 0,
                'total': 0
            },
            'news_dirs': 0,
            'last_updated': None
        }
        
        if data_dir.exists():
            for file in data_dir.glob("*.csv"):
                if file.name.startswith("daily_"):
                    status['market_files']['daily'] += 1
                elif file.name.startswith("hourly_"):
                    status['market_files']['hourly'] += 1
                status['market_files']['total'] += 1
        
        if news_dir.exists():
            status['news_dirs'] = len([d for d in news_dir.iterdir() if d.is_dir()])
        
        # æ‰¾æœ€æ–°æ›´æ–°æ™‚é–“
        if data_dir.exists():
            csv_files = list(data_dir.glob("*.csv"))
            if csv_files:
                latest_file = max(csv_files, key=lambda f: f.stat().st_mtime)
                status['last_updated'] = datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat()
        
        return status

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="æ•¸æ“šæ”¶é›†æ™ºå£«")
    parser.add_argument(
        "--market", 
        choices=["all", "taiwan", "us", "crypto"],
        default="all",
        help="æŒ‡å®šæ”¶é›†çš„å¸‚å ´æ•¸æ“š"
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="åŸ·è¡ŒèˆŠæ•¸æ“šæ¸…ç†"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="æ¸¬è©¦æ¨¡å¼ï¼ˆåªæ”¶é›†å°‘é‡æ•¸æ“šï¼‰"
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="é¡¯ç¤ºæ•¸æ“šç‹€æ…‹"
    )
    
    args = parser.parse_args()
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«å•Ÿå‹• ===")
    
    collector = DataCollector()
    
    if args.status:
        status = collector.get_data_status()
        logger.info("æ•¸æ“šç‹€æ…‹çµ±è¨ˆ:")
        logger.info(f"  å¸‚å ´æ•¸æ“šæª”æ¡ˆ: {status['market_files']['total']} å€‹ (æ—¥ç·š: {status['market_files']['daily']}, å°æ™‚ç·š: {status['market_files']['hourly']})")
        logger.info(f"  æ–°èç›®éŒ„: {status['news_dirs']} å€‹")
        if status['last_updated']:
            logger.info(f"  æœ€å¾Œæ›´æ–°: {status['last_updated']}")
        return
    
    if args.cleanup:
        collector.cleanup_old_data()
    
    if args.test:
        logger.info("æ¸¬è©¦æ¨¡å¼ï¼šåªæ”¶é›†å–®ä¸€è‚¡ç¥¨æ•¸æ“š")
        # æ¸¬è©¦æ¨¡å¼ä¸‹åªæ”¶é›†å°‘é‡æ•¸æ“š
        test_symbol = "^TWII" if args.market == "taiwan" else "^GSPC"
        data = collector.market_collector.fetch_yahoo_data(test_symbol)
        if data is not None:
            logger.success(f"æ¸¬è©¦æˆåŠŸï¼ç²å–åˆ° {len(data)} ç­† {test_symbol} æ•¸æ“š")
        else:
            logger.error("æ¸¬è©¦å¤±æ•—ï¼")
            sys.exit(1)
    else:
        # æ­£å¸¸æ¨¡å¼
        results = asyncio.run(collector.collect_all_data(args.market))
        
        if results['status'] == 'success':
            logger.success("âœ… æ•¸æ“šæ”¶é›†ä»»å‹™å®Œæˆï¼")
            if 'summary' in results:
                logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ: {results['summary']}")
        else:
            logger.error("âŒ æ•¸æ“šæ”¶é›†ä»»å‹™å¤±æ•—ï¼")
            sys.exit(1)
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«çµæŸ ===")

if __name__ == "__main__":
    main()
