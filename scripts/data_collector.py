"""
æ•¸æ“šæ”¶é›†æ™ºå£« (Data Intelligence Agent)
è² è²¬å¾å„ç¨®ä¾†æºæ”¶é›†å¸‚å ´æ•¸æ“šå’Œæ–°èè³‡è¨Š
"""

import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
import aiohttp
from typing import List, Dict, Optional, Tuple
import argparse
import sys

# å°å…¥æˆ‘å€‘çš„å·¥å…·å‡½æ•¸
from utils import (
    config_manager, 
    LoggerSetup, 
    retry_on_failure, 
    get_taiwan_time,
    validate_data_quality
)
from loguru import logger

# è¨­å®šæ—¥èªŒ
LoggerSetup.setup_logger("data_collector", config_manager.get("system.log_level", "INFO"))


class MarketDataCollector:
    """å¸‚å ´æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.config = config_manager
        # ç°¡åŒ–æ•¸æ“šç›®éŒ„çµæ§‹
        self.data_dir = Path("data/market")
        self.news_dir = Path("data/news")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.news_dir.mkdir(parents=True, exist_ok=True)
        
        # å‰µå»ºä»Šå¤©çš„æ–°èç›®éŒ„ï¼ˆæ–°èé‚„æ˜¯æŒ‰æ—¥æœŸåˆ†çµ„ï¼‰
        self.today = get_taiwan_time().strftime("%Y-%m-%d")
        self.today_news_dir = self.news_dir / self.today
        self.today_news_dir.mkdir(exist_ok=True)
    
    @retry_on_failure(max_retries=3, delay=3.0)
    def fetch_yahoo_data(
        self, 
        symbol: str, 
        period: str = "1y", 
        interval: str = "1d"
    ) -> Optional[pd.DataFrame]:
        """
        å¾ Yahoo Finance ç²å–è‚¡ç¥¨æ•¸æ“š
        
        Args:
            symbol: è‚¡ç¥¨ä»£è™Ÿ
            period: æ™‚é–“ç¯„åœ (1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max)
            interval: æ•¸æ“šé–“éš” (1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo)
        """
        try:
            logger.info(f"æ­£åœ¨ç²å– {symbol} çš„ {interval} æ•¸æ“šï¼Œç¯„åœ: {period}")
            
            ticker = yf.Ticker(symbol)
            data = ticker.history(period=period, interval=interval)
            
            if data.empty:
                logger.warning(f"{symbol} æ²’æœ‰ç²å–åˆ°æ•¸æ“š")
                return None
            
            # æ·»åŠ è‚¡ç¥¨ä»£è™Ÿå’Œæ›´æ–°æ™‚é–“æ¬„ä½
            data['Symbol'] = symbol
            data['Updated'] = get_taiwan_time().isoformat()
            
            # é‡è¨­ç´¢å¼•ï¼Œå°‡æ—¥æœŸä½œç‚ºæ¬„ä½
            data.reset_index(inplace=True)
            
            # é‡æ–°æ’åºæ¬„ä½ï¼Œè®“æ™‚é–“æ¬„ä½åœ¨å‰é¢
            columns = ['Datetime', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'Updated']
            if 'Dividends' in data.columns:
                columns.insert(-1, 'Dividends')
            if 'Stock Splits' in data.columns:
                columns.insert(-1, 'Stock Splits')
            
            data = data[columns]
            
            logger.success(f"æˆåŠŸç²å– {symbol} æ•¸æ“šï¼Œå…± {len(data)} ç­†è¨˜éŒ„")
            return data
            
        except Exception as e:
            logger.error(f"ç²å– {symbol} æ•¸æ“šå¤±æ•—: {str(e)}")
            raise
    
    def save_market_data(self, data: pd.DataFrame, symbol: str, data_type: str) -> bool:
        """
        å„²å­˜å¸‚å ´æ•¸æ“šåˆ°ç°¡åŒ–çš„æª”æ¡ˆçµæ§‹
        
        Args:
            data: DataFrame
            symbol: è‚¡ç¥¨ä»£è™Ÿ
            data_type: "daily" æˆ– "hourly"
        """
        try:
            # æ¸…ç†ç¬¦è™Ÿåç¨±ä½œç‚ºæª”æ¡ˆå
            clean_symbol = symbol.replace('^', '').replace('.', '_').replace('=', '_').replace('-', '_')
            filename = self.data_dir / f"{data_type}_{clean_symbol}.csv"
            
            # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œæª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if filename.exists():
                try:
                    existing_data = pd.read_csv(filename)
                    existing_data['Datetime'] = pd.to_datetime(existing_data['Datetime'])
                    data['Datetime'] = pd.to_datetime(data['Datetime'])
                    
                    # åˆä½µæ–°èˆŠæ•¸æ“šï¼Œå»é‡ä¸¦æ’åº
                    combined_data = pd.concat([existing_data, data]).drop_duplicates(
                        subset=['Datetime'], keep='last'
                    ).sort_values('Datetime').reset_index(drop=True)
                    
                    # æ ¹æ“šæ•¸æ“šé¡å‹ä¿ç•™é©ç•¶çš„è¨˜éŒ„æ•¸é‡
                    if data_type == "daily":
                        # ä¿ç•™æœ€è¿‘300å¤©
                        combined_data = combined_data.tail(300)
                    elif data_type == "hourly":
                        # ä¿ç•™æœ€è¿‘14å¤©çš„æ•¸æ“š
                        cutoff_date = get_taiwan_time() - timedelta(days=14)
                        combined_data = combined_data[combined_data['Datetime'] >= cutoff_date]
                    
                    data_to_save = combined_data
                    
                except Exception as e:
                    logger.warning(f"åˆä½µ {symbol} æ­·å²æ•¸æ“šæ™‚å‡ºéŒ¯ï¼Œä½¿ç”¨æ–°æ•¸æ“š: {str(e)}")
                    data_to_save = data
            else:
                data_to_save = data
            
            # å„²å­˜æ•¸æ“š
            data_to_save.to_csv(filename, index=False, encoding='utf-8')
            logger.success(f"å·²å„²å­˜ {symbol} {data_type} æ•¸æ“šåˆ° {filename} (å…± {len(data_to_save)} ç­†)")
            return True
            
        except Exception as e:
            logger.error(f"å„²å­˜ {symbol} {data_type} æ•¸æ“šå¤±æ•—: {str(e)}")
            return False
    
    def collect_taiwan_stocks(self) -> Dict[str, int]:
        """æ”¶é›†å°è‚¡æ•¸æ“š"""
        symbols = self.config.get("markets.taiwan.symbols", [])
        logger.info(f"é–‹å§‹æ”¶é›†å°è‚¡æ•¸æ“šï¼Œå…± {len(symbols)} æ”¯è‚¡ç¥¨")
        
        results = {'daily': 0, 'hourly': 0, 'errors': 0}
        
        for symbol in symbols:
            try:
                # æ”¶é›†æ—¥ç·šæ•¸æ“š
                daily_data = self.fetch_yahoo_data(symbol, period="1y", interval="1d")
                if daily_data is not None and validate_data_quality(daily_data, symbol):
                    if self.save_market_data(daily_data, symbol, "daily"):
                        results['daily'] += 1
                
                # æ”¶é›†å°æ™‚ç·šæ•¸æ“š
                hourly_data = self.fetch_yahoo_data(symbol, period="1mo", interval="1h")
                if hourly_data is not None and validate_data_quality(hourly_data, symbol, min_rows=5):
                    if self.save_market_data(hourly_data, symbol, "hourly"):
                        results['hourly'] += 1
                
            except Exception as e:
                logger.error(f"æ”¶é›† {symbol} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                results['errors'] += 1
                continue
        
        logger.info(f"å°è‚¡æ•¸æ“šæ”¶é›†å®Œæˆ - æ—¥ç·š: {results['daily']}, å°æ™‚ç·š: {results['hourly']}, éŒ¯èª¤: {results['errors']}")
        return results
    
    def collect_us_stocks(self) -> Dict[str, int]:
        """æ”¶é›†ç¾è‚¡æ•¸æ“š"""
        symbols = self.config.get("markets.us.symbols", [])
        logger.info(f"é–‹å§‹æ”¶é›†ç¾è‚¡æ•¸æ“šï¼Œå…± {len(symbols)} æ”¯è‚¡ç¥¨")
        
        results = {'daily': 0, 'hourly': 0, 'errors': 0}
        
        for symbol in symbols:
            try:
                # æ”¶é›†æ—¥ç·šæ•¸æ“š
                daily_data = self.fetch_yahoo_data(symbol, period="1y", interval="1d")
                if daily_data is not None and validate_data_quality(daily_data, symbol):
                    if self.save_market_data(daily_data, symbol, "daily"):
                        results['daily'] += 1
                
                # æ”¶é›†å°æ™‚ç·šæ•¸æ“š
                hourly_data = self.fetch_yahoo_data(symbol, period="1mo", interval="1h")
                if hourly_data is not None and validate_data_quality(hourly_data, symbol, min_rows=5):
                    if self.save_market_data(hourly_data, symbol, "hourly"):
                        results['hourly'] += 1
                
            except Exception as e:
                logger.error(f"æ”¶é›† {symbol} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                results['errors'] += 1
                continue
        
        logger.info(f"ç¾è‚¡æ•¸æ“šæ”¶é›†å®Œæˆ - æ—¥ç·š: {results['daily']}, å°æ™‚ç·š: {results['hourly']}, éŒ¯èª¤: {results['errors']}")
        return results
    
    def collect_crypto_data(self) -> Dict[str, int]:
        """æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š"""
        symbols = self.config.get("markets.crypto.symbols", [])
        logger.info(f"é–‹å§‹æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“šï¼Œå…± {len(symbols)} ç¨®")
        
        results = {'daily': 0, 'hourly': 0, 'errors': 0}
        
        for symbol in symbols:
            try:
                # æ”¶é›†æ—¥ç·šæ•¸æ“š
                daily_data = self.fetch_yahoo_data(symbol, period="6mo", interval="1d")
                if daily_data is not None and validate_data_quality(daily_data, symbol):
                    if self.save_market_data(daily_data, symbol, "daily"):
                        results['daily'] += 1
                
                # æ”¶é›†å°æ™‚ç·šæ•¸æ“š
                hourly_data = self.fetch_yahoo_data(symbol, period="1mo", interval="1h")
                if hourly_data is not None and validate_data_quality(hourly_data, symbol, min_rows=5):
                    if self.save_market_data(hourly_data, symbol, "hourly"):
                        results['hourly'] += 1
                
            except Exception as e:
                logger.error(f"æ”¶é›† {symbol} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                results['errors'] += 1
                continue
        
        logger.info(f"åŠ å¯†è²¨å¹£æ•¸æ“šæ”¶é›†å®Œæˆ - æ—¥ç·š: {results['daily']}, å°æ™‚ç·š: {results['hourly']}, éŒ¯èª¤: {results['errors']}")
        return results


class NewsDataCollector:
    """æ–°èæ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.config = config_manager
        self.news_dir = Path("data/news")
        self.news_dir.mkdir(parents=True, exist_ok=True)
        
        self.today = get_taiwan_time().strftime("%Y-%m-%d")
        self.today_dir = self.news_dir / self.today
        self.today_dir.mkdir(exist_ok=True)
    
    @retry_on_failure(max_retries=3, delay=2.0)
    def fetch_rss_news(self, rss_url: str, max_items: int = 10) -> List[Dict]:
        """
        å¾ RSS ç²å–æ–°è
        
        Args:
            rss_url: RSS ç¶²å€
            max_items: æœ€å¤§æ–°èæ•¸é‡
        """
        try:
            logger.info(f"æ­£åœ¨ç²å– RSS æ–°è: {rss_url}")
            
            feed = feedparser.parse(rss_url)
            
            if feed.bozo:
                logger.warning(f"RSS feed å¯èƒ½æœ‰æ ¼å¼å•é¡Œ: {rss_url}")
            
            news_items = []
            for entry in feed.entries[:max_items]:
                news_item = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'summary': entry.get('summary', ''),
                    'source': feed.feed.get('title', 'Unknown'),
                    'collected_at': get_taiwan_time().isoformat()
                }
                news_items.append(news_item)
            
            logger.success(f"æˆåŠŸç²å– {len(news_items)} å‰‡æ–°è")
            return news_items
            
        except Exception as e:
            logger.error(f"ç²å– RSS æ–°èå¤±æ•—: {str(e)}")
            raise
    
    def collect_taiwan_news(self) -> int:
        """æ”¶é›†å°è‚¡ç›¸é—œæ–°è"""
        news_sources = self.config.get("markets.taiwan.news_sources", [])
        all_news = []
        
        for source_url in news_sources:
            try:
                news = self.fetch_rss_news(source_url, max_items=5)
                all_news.extend(news)
            except Exception as e:
                logger.error(f"æ”¶é›†å°è‚¡æ–°èå¤±æ•— {source_url}: {str(e)}")
                continue
        
        if all_news:
            # å„²å­˜æ–°èæ•¸æ“š
            import json
            filename = self.today_dir / "taiwan_news.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(all_news, f, ensure_ascii=False, indent=2)
            logger.success(f"å·²å„²å­˜ {len(all_news)} å‰‡å°è‚¡æ–°èåˆ° {filename}")
        
        return len(all_news)
    
    def collect_us_news(self) -> int:
        """æ”¶é›†ç¾è‚¡ç›¸é—œæ–°è"""
        news_sources = self.config.get("markets.us.news_sources", [])
        all_news = []
        
        for source_url in news_sources:
            try:
                news = self.fetch_rss_news(source_url, max_items=5)
                all_news.extend(news)
            except Exception as e:
                logger.error(f"æ”¶é›†ç¾è‚¡æ–°èå¤±æ•— {source_url}: {str(e)}")
                continue
        
        if all_news:
            import json
            filename = self.today_dir / "us_news.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(all_news, f, ensure_ascii=False, indent=2)
            logger.success(f"å·²å„²å­˜ {len(all_news)} å‰‡ç¾è‚¡æ–°èåˆ° {filename}")
        
        return len(all_news)


class DataCollector:
    """ä¸»æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.market_collector = MarketDataCollector()
        self.news_collector = NewsDataCollector()
    
    def collect_all_data(self, market: str = "all") -> Dict:
        """
        æ”¶é›†æ‰€æœ‰æ•¸æ“š
        
        Args:
            market: "all", "taiwan", "us", "crypto"
        """
        logger.info(f"é–‹å§‹æ•¸æ“šæ”¶é›†ä»»å‹™ï¼Œå¸‚å ´ç¯„åœ: {market}")
        start_time = get_taiwan_time()
        
        results = {
            'market_data': {},
            'news_data': {},
            'collection_time': start_time.isoformat(),
            'status': 'success',
            'summary': {}
        }
        
        try:
            total_files = 0
            
            # æ”¶é›†å¸‚å ´æ•¸æ“š
            if market in ["all", "taiwan"]:
                logger.info("æ”¶é›†å°è‚¡æ•¸æ“š...")
                taiwan_data = self.market_collector.collect_taiwan_stocks()
                results['market_data']['taiwan'] = taiwan_data
                total_files += taiwan_data['daily'] + taiwan_data['hourly']
                
                logger.info("æ”¶é›†å°è‚¡æ–°è...")
                taiwan_news_count = self.news_collector.collect_taiwan_news()
                results['news_data']['taiwan'] = taiwan_news_count
            
            if market in ["all", "us"]:
                logger.info("æ”¶é›†ç¾è‚¡æ•¸æ“š...")
                us_data = self.market_collector.collect_us_stocks()
                results['market_data']['us'] = us_data
                total_files += us_data['daily'] + us_data['hourly']
                
                logger.info("æ”¶é›†ç¾è‚¡æ–°è...")
                us_news_count = self.news_collector.collect_us_news()
                results['news_data']['us'] = us_news_count
            
            if market in ["all", "crypto"]:
                logger.info("æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š...")
                crypto_data = self.market_collector.collect_crypto_data()
                results['market_data']['crypto'] = crypto_data
                total_files += crypto_data['daily'] + crypto_data['hourly']
            
            end_time = get_taiwan_time()
            duration = (end_time - start_time).total_seconds()
            
            results['duration_seconds'] = duration
            results['completion_time'] = end_time.isoformat()
            results['summary']['total_market_files'] = total_files
            
            logger.success(f"æ•¸æ“šæ”¶é›†å®Œæˆï¼Œè€—æ™‚ {duration:.2f} ç§’ï¼Œå…±è™•ç† {total_files} å€‹å¸‚å ´æ•¸æ“šæª”æ¡ˆ")
            
        except Exception as e:
            logger.error(f"æ•¸æ“šæ”¶é›†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            results['status'] = 'error'
            results['error_message'] = str(e)
        
        return results
    
    def cleanup_old_data(self, retention_days: int = 30):
        """æ¸…ç†èˆŠæ•¸æ“š"""
        logger.info(f"é–‹å§‹æ¸…ç† {retention_days} å¤©å‰çš„èˆŠæ•¸æ“š")
        
        cutoff_date = get_taiwan_time() - timedelta(days=retention_days)
        cutoff_str = cutoff_date.strftime("%Y-%m-%d")
        
        # åªæ¸…ç†æ–°èæ•¸æ“šï¼ˆæŒ‰æ—¥æœŸåˆ†çµ„ï¼‰
        # å¸‚å ´æ•¸æ“šæª”æ¡ˆç¾åœ¨æ˜¯ç›´æ¥æ›´æ–°ï¼Œä¸éœ€è¦æŒ‰æ—¥æœŸæ¸…ç†
        news_dir = Path("data/news")
        if news_dir.exists():
            cleaned_dirs = 0
            for date_dir in news_dir.iterdir():
                if date_dir.is_dir() and date_dir.name < cutoff_str:
                    import shutil
                    shutil.rmtree(date_dir)
                    cleaned_dirs += 1
                    logger.info(f"å·²åˆªé™¤èˆŠæ–°èç›®éŒ„: {date_dir}")
            
            if cleaned_dirs == 0:
                logger.info("æ²’æœ‰éœ€è¦æ¸…ç†çš„èˆŠæ–°èæ•¸æ“š")
            else:
                logger.success(f"å·²æ¸…ç† {cleaned_dirs} å€‹èˆŠæ–°èç›®éŒ„")
    
    def get_data_status(self) -> Dict:
        """ç²å–æ•¸æ“šç‹€æ…‹çµ±è¨ˆ"""
        data_dir = Path("data/market")
        news_dir = Path("data/news")
        
        status = {
            'market_files': {
                'daily': 0,
                'hourly': 0,
                'total': 0
            },
            'news_dirs': 0,
            'last_updated': None
        }
        
        if data_dir.exists():
            for file in data_dir.glob("*.csv"):
                if file.name.startswith("daily_"):
                    status['market_files']['daily'] += 1
                elif file.name.startswith("hourly_"):
                    status['market_files']['hourly'] += 1
                status['market_files']['total'] += 1
        
        if news_dir.exists():
            status['news_dirs'] = len([d for d in news_dir.iterdir() if d.is_dir()])
        
        # æ‰¾æœ€æ–°æ›´æ–°æ™‚é–“
        if data_dir.exists():
            csv_files = list(data_dir.glob("*.csv"))
            if csv_files:
                latest_file = max(csv_files, key=lambda f: f.stat().st_mtime)
                status['last_updated'] = datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat()
        
        return status


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="æ•¸æ“šæ”¶é›†æ™ºå£«")
    parser.add_argument(
        "--market", 
        choices=["all", "taiwan", "us", "crypto"],
        default="all",
        help="æŒ‡å®šæ”¶é›†çš„å¸‚å ´æ•¸æ“š"
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="åŸ·è¡ŒèˆŠæ•¸æ“šæ¸…ç†"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="æ¸¬è©¦æ¨¡å¼ï¼ˆåªæ”¶é›†å°‘é‡æ•¸æ“šï¼‰"
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="é¡¯ç¤ºæ•¸æ“šç‹€æ…‹"
    )
    
    args = parser.parse_args()
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«å•Ÿå‹• ===")
    
    collector = DataCollector()
    
    if args.status:
        status = collector.get_data_status()
        logger.info("æ•¸æ“šç‹€æ…‹çµ±è¨ˆ:")
        logger.info(f"  å¸‚å ´æ•¸æ“šæª”æ¡ˆ: {status['market_files']['total']} å€‹ (æ—¥ç·š: {status['market_files']['daily']}, å°æ™‚ç·š: {status['market_files']['hourly']})")
        logger.info(f"  æ–°èç›®éŒ„: {status['news_dirs']} å€‹")
        if status['last_updated']:
            logger.info(f"  æœ€å¾Œæ›´æ–°: {status['last_updated']}")
        return
    
    if args.cleanup:
        collector.cleanup_old_data()
    
    if args.test:
        logger.info("æ¸¬è©¦æ¨¡å¼ï¼šåªæ”¶é›†å–®ä¸€è‚¡ç¥¨æ•¸æ“š")
        # æ¸¬è©¦æ¨¡å¼ä¸‹åªæ”¶é›†å°‘é‡æ•¸æ“š
        test_symbol = "^TWII" if args.market == "taiwan" else "^GSPC"
        data = collector.market_collector.fetch_yahoo_data(test_symbol)
        if data is not None:
            logger.success(f"æ¸¬è©¦æˆåŠŸï¼ç²å–åˆ° {len(data)} ç­† {test_symbol} æ•¸æ“š")
        else:
            logger.error("æ¸¬è©¦å¤±æ•—ï¼")
            sys.exit(1)
    else:
        # æ­£å¸¸æ¨¡å¼
        results = collector.collect_all_data(args.market)
        
        if results['status'] == 'success':
            logger.success("âœ… æ•¸æ“šæ”¶é›†ä»»å‹™å®Œæˆï¼")
            if 'summary' in results:
                logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ: {results['summary']}")
        else:
            logger.error("âŒ æ•¸æ“šæ”¶é›†ä»»å‹™å¤±æ•—ï¼")
            sys.exit(1)
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«çµæŸ ===")


if __name__ == "__main__":
    main()
