import yfinance as yf
import pandas as pd
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
import aiohttp
import json  # æ–°å¢å°å…¥
from typing import List, Dict, Optional, Tuple
import argparse
import sys

# å°å…¥æˆ‘å€‘çš„å·¥å…·å‡½æ•¸
from utils import (
    config_manager, 
    LoggerSetup, 
    retry_on_failure, 
    get_taiwan_time,
    validate_data_quality
)
from loguru import logger

# è¨­å®šæ—¥èªŒ
LoggerSetup.setup_logger("data_collector", config_manager.get("system.log_level", "INFO"))

class MarketDataCollector:
    """å¸‚å ´æ•¸æ“šæ”¶é›†å™¨"""
    
    def __init__(self):
        self.config = config_manager
        self.data_dir = Path("data/market")
        self.news_dir = Path("data/news")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.news_dir.mkdir(parents=True, exist_ok=True)
        
        self.today = get_taiwan_time().strftime("%Y-%m-%d")
        self.today_news_dir = self.news_dir / self.today
        self.today_news_dir.mkdir(exist_ok=True)
    
    @retry_on_failure(max_retries=3, delay=3.0)
    def fetch_yahoo_data(
        self, 
        symbol: str, 
        period: str = "1y", 
        interval: str = "1d"
    ) -> Optional[pd.DataFrame]:
        """
        å¾ Yahoo Finance ç²å–è‚¡ç¥¨æ•¸æ“š
        """
        try:
            logger.info(f"æ­£åœ¨ç²å– {symbol} çš„ {interval} æ•¸æ“šï¼Œç¯„åœ: {period}")
            ticker = yf.Ticker(symbol)
            data = ticker.history(period=period, interval=interval)
            
            if data.empty:
                logger.warning(f"{symbol} æ²’æœ‰ç²å–åˆ°æ•¸æ“š")
                return None
            
            data['Symbol'] = symbol
            data['Updated'] = get_taiwan_time().isoformat()
            data.reset_index(inplace=True)
            
            if 'Date' in data.columns:
                data.rename(columns={'Date': 'Datetime'}, inplace=True)
            
            required_columns = ['Datetime', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume']
            missing_columns = [col for col in required_columns if col not in data.columns]
            if missing_columns:
                logger.error(f"{symbol} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            columns = ['Datetime', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'Updated']
            optional_columns = ['Dividends', 'Stock Splits']
            for col in optional_columns:
                if col in data.columns:
                    columns.insert(-1, col)
            
            available_columns = [col for col in columns if col in data.columns]
            data = data[available_columns]
            
            logger.success(f"æˆåŠŸç²å– {symbol} æ•¸æ“šï¼Œå…± {len(data)} ç­†è¨˜éŒ„")
            return data
            
        except Exception as e:
            logger.error(f"ç²å– {symbol} æ•¸æ“šå¤±æ•—: {str(e)}")
            raise
    
    def save_market_data(self, data: pd.DataFrame, symbol: str, data_type: str) -> bool:
        """
        å„²å­˜å¸‚å ´æ•¸æ“šåˆ°ç°¡åŒ–çš„æª”æ¡ˆçµæ§‹
        """
        try:
            clean_symbol = symbol.replace('^', '').replace('.', '_').replace('=', '_').replace('-', '_')
            filename = self.data_dir / f"{data_type}_{clean_symbol}.csv"
            
            if filename.exists():
                try:
                    existing_data = pd.read_csv(filename)
                    existing_data['Datetime'] = pd.to_datetime(existing_data['Datetime'])
                    data['Datetime'] = pd.to_datetime(data['Datetime'])
                    combined_data = pd.concat([existing_data, data]).drop_duplicates(
                        subset=['Datetime'], keep='last'
                    ).sort_values('Datetime').reset_index(drop=True)
                    
                    if data_type == "daily":
                        combined_data = combined_data.tail(300)
                    elif data_type == "hourly":
                        cutoff_date = get_taiwan_time() - timedelta(days=14)
                        combined_data = combined_data[combined_data['Datetime'] >= cutoff_date]
                    
                    data_to_save = combined_data
                except Exception as e:
                    logger.warning(f"åˆä½µ {symbol} æ­·å²æ•¸æ“šæ™‚å‡ºéŒ¯ï¼Œä½¿ç”¨æ–°æ•¸æ“š: {str(e)}")
                    data_to_save = data
            else:
                data_to_save = data
            
            data_to_save.to_csv(filename, index=False)
            logger.info(f"æˆåŠŸå„²å­˜ {symbol} çš„ {data_type} æ•¸æ“šåˆ° {filename}")
            return True
            
        except Exception as e:
            logger.error(f"å„²å­˜ {symbol} çš„ {data_type} æ•¸æ“šå¤±æ•—: {str(e)}")
            return False
    
    async def fetch_news(self, url: str, category: str) -> List[Dict]:
        """ç•°æ­¥æŠ“å–æ–°è RSS"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(url, timeout=10) as response:
                    text = await response.text()
                    feed = feedparser.parse(text)
                    
                    news_items = []
                    for entry in feed.entries[:self.config.get("data_collection.news_limit", 3)]:
                        news_items.append({
                            'title': entry.get('title', ''),
                            'link': entry.get('link', ''),
                            'published': entry.get('published', ''),
                            'summary': entry.get('summary', ''),
                            'category': category
                        })
                    
                    return news_items
                    
        except Exception as e:
            logger.error(f"æŠ“å–æ–°è RSS å¤±æ•— ({url}): {str(e)}")
            return []

class NewsCollector:
    """æ–°èæ”¶é›†å™¨"""
    
    def __init__(self, market_collector: MarketDataCollector):
        self.market_collector = market_collector
        self.config = market_collector.config
    
    async def collect_taiwan_news(self) -> int:
        """æ”¶é›†å°è‚¡æ–°è"""
        news_urls = self.config.get("markets.taiwan.news_sources", [])
        categories = self.config.get("news_categories", ["ç¶“æ¿Ÿ", "åŠå°é«”"])
        news_count = 0
        
        for url in news_urls:
            for category in categories:
                try:
                    news_items = await self.market_collector.fetch_news(url, category)
                    for item in news_items:
                        filename = self.market_collector.today_news_dir / f"news_taiwan_{news_count}.json"
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(item, f, ensure_ascii=False, indent=2)
                        news_count += 1
                        logger.info(f"ä¿å­˜å°è‚¡æ–°è: {filename}")
                except Exception as e:
                    logger.error(f"è™•ç†æ–°è {url} (é¡åˆ¥: {category}) å¤±æ•—: {str(e)}")
        
        logger.info(f"å…±æ”¶é›† {news_count} ç¯‡å°è‚¡æ–°è")
        return news_count
    
    async def collect_us_news(self) -> int:
        """æ”¶é›†ç¾è‚¡æ–°è"""
        news_urls = self.config.get("markets.us.news_sources", [])
        categories = self.config.get("news_categories", ["ç¶“æ¿Ÿ", "åŠå°é«”"])
        news_count = 0
        
        for url in news_urls:
            for category in categories:
                try:
                    news_items = await self.market_collector.fetch_news(url, category)
                    for item in news_items:
                        filename = self.market_collector.today_news_dir / f"news_us_{news_count}.json"
                        with open(filename, 'w', encoding='utf-8') as f:
                            json.dump(item, f, ensure_ascii=False, indent=2)
                        news_count += 1
                        logger.info(f"ä¿å­˜ç¾è‚¡æ–°è: {filename}")
                except Exception as e:
                    logger.error(f"è™•ç†æ–°è {url} (é¡åˆ¥: {category}) å¤±æ•—: {str(e)}")
        
        logger.info(f"å…±æ”¶é›† {news_count} ç¯‡ç¾è‚¡æ–°è")
        return news_count

class DataCollector:
    """æ•´é«”æ•¸æ“šæ”¶é›†å”èª¿å™¨"""
    
    def __init__(self):
        self.market_collector = MarketDataCollector()
        self.news_collector = NewsCollector(self.market_collector)
    
    async def collect_all_data(self, market: str = "all") -> Dict:
        """æ”¶é›†æ‰€æœ‰æŒ‡å®šå¸‚å ´çš„æ•¸æ“š"""
        results = {
            'status': 'success',
            'market_data': {'taiwan': {}, 'us': {}, 'crypto': {}},
            'news_data': {'taiwan': 0, 'us': 0},
            'summary': {'total_market_files': 0},
            'error_message': None
        }
        
        start_time = get_taiwan_time()
        total_files = 0
        
        try:
            if market in ["all", "taiwan"]:
                logger.info("æ”¶é›†å°è‚¡æ•¸æ“š...")
                taiwan_data = self.market_collector.collect_taiwan_stocks()
                results['market_data']['taiwan'] = taiwan_data
                total_files += taiwan_data['daily'] + taiwan_data['hourly']
                
                logger.info("æ”¶é›†å°è‚¡æ–°è...")
                taiwan_news_count = await self.news_collector.collect_taiwan_news()
                results['news_data']['taiwan'] = taiwan_news_count
            
            if market in ["all", "us"]:
                logger.info("æ”¶é›†ç¾è‚¡æ•¸æ“š...")
                us_data = self.market_collector.collect_us_stocks()
                results['market_data']['us'] = us_data
                total_files += us_data['daily'] + us_data['hourly']
                
                logger.info("æ”¶é›†ç¾è‚¡æ–°è...")
                us_news_count = await self.news_collector.collect_us_news()
                results['news_data']['us'] = us_news_count
            
            if market in ["all", "crypto"]:
                logger.info("æ”¶é›†åŠ å¯†è²¨å¹£æ•¸æ“š...")
                crypto_data = self.market_collector.collect_crypto_data()
                results['market_data']['crypto'] = crypto_data
                total_files += crypto_data['daily'] + crypto_data['hourly']
            
            end_time = get_taiwan_time()
            duration = (end_time - start_time).total_seconds()
            
            results['duration_seconds'] = duration
            results['completion_time'] = end_time.isoformat()
            results['summary']['total_market_files'] = total_files
            
            logger.success(f"æ•¸æ“šæ”¶é›†å®Œæˆï¼Œè€—æ™‚ {duration:.2f} ç§’ï¼Œå…±è™•ç† {total_files} å€‹å¸‚å ´æ•¸æ“šæª”æ¡ˆ")
            
        except Exception as e:
            logger.error(f"æ•¸æ“šæ”¶é›†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            results['status'] = 'error'
            results['error_message'] = str(e)
        
        return results
    
    def cleanup_old_data(self, retention_days: int = 30):
        """æ¸…ç†èˆŠæ•¸æ“š"""
        logger.info(f"é–‹å§‹æ¸…ç† {retention_days} å¤©å‰çš„èˆŠæ•¸æ“š")
        
        cutoff_date = get_taiwan_time() - timedelta(days=retention_days)
        cutoff_str = cutoff_date.strftime("%Y-%m-%d")
        
        news_dir = Path("data/news")
        if news_dir.exists():
            cleaned_dirs = 0
            for date_dir in news_dir.iterdir():
                if date_dir.is_dir() and date_dir.name < cutoff_str:
                    import shutil
                    shutil.rmtree(date_dir)
                    cleaned_dirs += 1
                    logger.info(f"å·²åˆªé™¤èˆŠæ–°èç›®éŒ„: {date_dir}")
            
            if cleaned_dirs == 0:
                logger.info("æ²’æœ‰éœ€è¦æ¸…ç†çš„èˆŠæ–°èæ•¸æ“š")
            else:
                logger.success(f"å·²æ¸…ç† {cleaned_dirs} å€‹èˆŠæ–°èç›®éŒ„")
    
    def get_data_status(self) -> Dict:
        """ç²å–æ•¸æ“šç‹€æ…‹çµ±è¨ˆ"""
        data_dir = Path("data/market")
        news_dir = Path("data/news")
        
        status = {
            'market_files': {
                'daily': 0,
                'hourly': 0,
                'total': 0
            },
            'news_dirs': 0,
            'last_updated': None
        }
        
        if data_dir.exists():
            for file in data_dir.glob("*.csv"):
                if file.name.startswith("daily_"):
                    status['market_files']['daily'] += 1
                elif file.name.startswith("hourly_"):
                    status['market_files']['hourly'] += 1
                status['market_files']['total'] += 1
        
        if news_dir.exists():
            status['news_dirs'] = len([d for d in news_dir.iterdir() if d.is_dir()])
        
        if data_dir.exists():
            csv_files = list(data_dir.glob("*.csv"))
            if csv_files:
                latest_file = max(csv_files, key=lambda f: f.stat().st_mtime)
                status['last_updated'] = datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat()
        
        return status

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description="æ•¸æ“šæ”¶é›†æ™ºå£«")
    parser.add_argument(
        "--market", 
        choices=["all", "taiwan", "us", "crypto"],
        default="all",
        help="æŒ‡å®šæ”¶é›†çš„å¸‚å ´æ•¸æ“š"
    )
    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="åŸ·è¡ŒèˆŠæ•¸æ“šæ¸…ç†"
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="æ¸¬è©¦æ¨¡å¼ï¼ˆåªæ”¶é›†å°‘é‡æ•¸æ“šï¼‰"
    )
    parser.add_argument(
        "--status",
        action="store_true",
        help="é¡¯ç¤ºæ•¸æ“šç‹€æ…‹"
    )
    
    args = parser.parse_args()
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«å•Ÿå‹• ===")
    
    collector = DataCollector()
    
    if args.status:
        status = collector.get_data_status()
        logger.info("æ•¸æ“šç‹€æ…‹çµ±è¨ˆ:")
        logger.info(f"  å¸‚å ´æ•¸æ“šæª”æ¡ˆ: {status['market_files']['total']} å€‹ (æ—¥ç·š: {status['market_files']['daily']}, å°æ™‚ç·š: {status['market_files']['hourly']})")
        logger.info(f"  æ–°èç›®éŒ„: {status['news_dirs']} å€‹")
        if status['last_updated']:
            logger.info(f"  æœ€å¾Œæ›´æ–°: {status['last_updated']}")
        return
    
    if args.cleanup:
        collector.cleanup_old_data()
    
    if args.test:
        logger.info("æ¸¬è©¦æ¨¡å¼ï¼šåªæ”¶é›†å–®ä¸€è‚¡ç¥¨æ•¸æ“š")
        test_symbol = "^TWII" if args.market == "taiwan" else "^GSPC"
        data = collector.market_collector.fetch_yahoo_data(test_symbol)
        if data is not None:
            logger.success(f"æ¸¬è©¦æˆåŠŸï¼ç²å–åˆ° {len(data)} ç­† {test_symbol} æ•¸æ“š")
        else:
            logger.error("æ¸¬è©¦å¤±æ•—ï¼")
            sys.exit(1)
    else:
        results = asyncio.run(collector.collect_all_data(args.market))
        
        if results['status'] == 'success':
            logger.success("âœ… æ•¸æ“šæ”¶é›†ä»»å‹™å®Œæˆï¼")
            if 'summary' in results:
                logger.info(f"ğŸ“Š è™•ç†çµ±è¨ˆ: {results['summary']}")
        else:
            logger.error("âŒ æ•¸æ“šæ”¶é›†ä»»å‹™å¤±æ•—ï¼")
            sys.exit(1)
    
    logger.info("=== æ•¸æ“šæ”¶é›†æ™ºå£«çµæŸ ===")

if __name__ == "__main__":
    main()


