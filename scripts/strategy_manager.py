import pandas as pd
import json
import os
import sys
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from statsmodels.tsa.arima.model import ARIMA
from utils import LoggerSetup, retry_on_failure, get_taiwan_time, config_manager, slack_alert
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from openai import OpenAI
from datetime import datetime, timedelta
from loguru import logger
import logging

logger = LoggerSetup.setup_logger('strategy_manager', config_manager.get("system.log_level", "INFO"))

def get_grok_client():
    """ç²å– Grok API å®¢æˆ¶ç«¯"""
    api_key = config_manager.get_secret('api_keys.grok_api_key')
    if not api_key:
        raise ValueError("æœªé…ç½® GROK_API_KEY")
    return OpenAI(
        api_key=api_key,
        base_url=config_manager.get('llm.grok_api_url', "https://api.grok.xai.com/v1")
    )

@retry_on_failure(max_retries=3, delay=3.0)
def optimize_params(strategy_name, params, performance):
    """ä½¿ç”¨ Grok API å„ªåŒ–ç­–ç•¥åƒæ•¸"""
    try:
        client = get_grok_client()
        prompt = f"""
        è«‹ç‚º {strategy_name} ç­–ç•¥å„ªåŒ–åƒæ•¸ï¼Œç”¨æ–¼çŸ­æœŸäº¤æ˜“ï¼ˆ1-3å¤©ï¼‰ã€‚
        ç•¶å‰è¡¨ç¾: {performance}
        ç•¶å‰åƒæ•¸: {json.dumps(params, indent=2)}
        
        è«‹è¿”å›å„ªåŒ–å¾Œçš„åƒæ•¸ï¼Œæ ¼å¼ç‚ºJSONï¼š
        {{
            "param1": value1,
            "param2": value2,
            ...
        }}
        """
        
        response = client.chat.completions.create(
            model=config_manager.get('llm.grok_model', "grok-3-mini"),
            messages=[{"role": "user", "content": prompt}],
            temperature=config_manager.get('llm.temperature', 0.6),
            max_tokens=config_manager.get('llm.max_tokens', 2200)
        )
        
        response_text = response.choices[0].message.content.strip()
        import re
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            optimized_params = json.loads(json_match.group())
            logger.info(f"æˆåŠŸå„ªåŒ– {strategy_name} ç­–ç•¥åƒæ•¸")
            return optimized_params
        else:
            logger.warning(f"ç„¡æ³•è§£æ Grok API å›æ‡‰ï¼Œä½¿ç”¨åŸå§‹åƒæ•¸")
            return params
            
    except Exception as e:
        logger.error(f"å„ªåŒ–ç­–ç•¥åƒæ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return params

def load_strategies_config():
    """è¼‰å…¥ç­–ç•¥é…ç½®"""
    try:
        strategies_config = config_manager.strategies_config
        if not strategies_config or 'available_strategies' not in strategies_config:
            logger.error("ç­–ç•¥é…ç½®æª”æ¡ˆæ ¼å¼éŒ¯èª¤æˆ–ç‚ºç©º")
            return []
        
        strategies = []
        for strategy_key, strategy_info in strategies_config['available_strategies'].items():
            if strategy_info.get('enabled', False):
                strategy = {
                    'name': strategy_key,
                    'display_name': strategy_info.get('name', strategy_key),
                    'params': strategy_info.get('parameters', {}),
                    'weight': strategy_info.get('weight', 0.25)
                }
                strategies.append(strategy)
        
        logger.info(f"è¼‰å…¥ {len(strategies)} å€‹å•Ÿç”¨çš„ç­–ç•¥")
        return strategies
        
    except Exception as e:
        logger.error(f"è¼‰å…¥ç­–ç•¥é…ç½®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def load_news_sentiment(symbol: str, date: str) -> float:
    """å¾æ–°èæ•¸æ“šè¨ˆç®—æƒ…ç·’åˆ†æ•¸ï¼ˆæ¨¡æ“¬ï¼‰"""
    try:
        news_dir = Path("data/news") / date
        if not news_dir.exists():
            logger.warning(f"æ–°èç›®éŒ„ {news_dir} ä¸å­˜åœ¨")
            return 0.0
        
        sentiment_scores = []
        for news_file in news_dir.glob(f"news_taiwan_*.json"):
            try:
                with open(news_file, 'r', encoding='utf-8') as f:
                    news = json.load(f)
                # æ¨¡æ“¬æƒ…ç·’åˆ†æï¼ˆå¯¦éš›æ‡‰ä½¿ç”¨ Grok APIï¼‰
                summary = news.get('summary', '')
                client = get_grok_client()
                prompt = f"åˆ†æä»¥ä¸‹æ–°èæ‘˜è¦çš„æƒ…ç·’ï¼ˆæ­£å‘/è² å‘ï¼Œç¯„åœ -1 åˆ° 1ï¼‰ï¼š\n{summary}"
                response = client.chat.completions.create(
                    model=config_manager.get('llm.grok_model', "grok-3-mini"),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.5,
                    max_tokens=100
                )
                score = float(response.choices[0].message.content.strip()) if response.choices[0].message.content.strip().replace('.', '').isdigit() else 0.0
                sentiment_scores.append(score)
            except Exception as e:
                logger.warning(f"è™•ç†æ–°è {news_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return np.mean(sentiment_scores) if sentiment_scores else 0.0
    
    except Exception as e:
        logger.error(f"è¨ˆç®— {symbol} æƒ…ç·’åˆ†æ•¸å¤±æ•—: {e}")
        return 0.0

def validate_data_quality(df, symbol: str, min_rows: int = 100) -> bool:
    """é©—è­‰æ•¸æ“šå“è³ª"""
    if df is None or len(df) < min_rows:
        logger.warning(f"{symbol} æ•¸æ“šè¡Œæ•¸ä¸è¶³: {len(df) if df is not None else 0}")
        return False
    
    required_columns = ['Datetime', 'Open', 'High', 'Low', 'Close', 'Volume']
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        logger.warning(f"{symbol} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
        return False
    
    null_percentage = df.isnull().sum().sum() / (len(df) * len(df.columns))
    if null_percentage > 0.1:
        logger.warning(f"{symbol} ç©ºå€¼æ¯”ä¾‹éé«˜: {null_percentage:.2%}")
        return False
    
    if (df['Close'] <= 0).any():
        logger.warning(f"{symbol} å­˜åœ¨ç•°å¸¸åƒ¹æ ¼æ•¸æ“š")
        return False
    
    logger.info(f"{symbol} æ•¸æ“šå“è³ªé©—è­‰é€šéï¼Œå…± {len(df)} ç­†è¨˜éŒ„")
    return True

def main(mode=None):
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    try:
        base_config = config_manager.base_config
        strategies = load_strategies_config()
        
        if not strategies:
            slack_alert("ç„¡å¯ç”¨çš„ç­–ç•¥é…ç½®", urgent=True)
            return
        
        if mode == 'us':
            focus_symbol = 'QQQ'
            market_config = base_config.get('markets', {}).get('us', {})
        else:
            focus_symbol = '0050.TW'
            market_config = base_config.get('markets', {}).get('taiwan', {})
        
        clean_symbol = focus_symbol.replace('.', '_').replace('^', '')
        
        project_root = Path(__file__).parent.parent
        daily_file = project_root / "data/market" / f"daily_{clean_symbol}.csv"
        
        if not daily_file.exists():
            logger.error(f"æ‰¾ä¸åˆ°æ•¸æ“šæª”æ¡ˆ: {daily_file}")
            slack_alert(f"ç¼ºå°‘ {focus_symbol} çš„æ•¸æ“šæª”æ¡ˆ: {daily_file}")
            return
        
        logger.info(f"è¼‰å…¥æ•¸æ“šæª”æ¡ˆ: {daily_file}")
        df = pd.read_csv(daily_file, parse_dates=['Datetime'])
        
        if not validate_data_quality(df, focus_symbol):
            slack_alert(f"{focus_symbol} æ•¸æ“šå“è³ªä¸åˆæ ¼")
            return
        
        # è¨­ç½®ç´¢å¼•ä¸¦è¨ˆç®—ç‰¹å¾µ
        df.set_index('Datetime', inplace=True)
        df['Return'] = df['Close'].pct_change()
        df['Label'] = np.where(df['Return'] > 0, 1, 0)
        df['SMA_5'] = df['Close'].rolling(window=5).mean()
        df['SMA_20'] = df['Close'].rolling(window=20).mean()
        df['RSI'] = calculate_rsi(df['Close'], window=14)
        df['Volume_MA'] = df['Volume'].rolling(window=20).mean()
        df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']
        
        # æ·»åŠ æƒ…ç·’åˆ†æ•¸
        today = get_taiwan_time().strftime("%Y-%m-%d")
        df['Sentiment'] = load_news_sentiment(focus_symbol, today)
        
        df.dropna(inplace=True)
        
        if len(df) < 50:
            logger.warning(f"è™•ç†å¾Œæ•¸æ“šé‡ä¸è¶³: {len(df)}")
            slack_alert(f"{focus_symbol} è™•ç†å¾Œæ•¸æ“šé‡ä¸è¶³")
            return
        
        logger.info(f"{focus_symbol} æ•¸æ“šé è™•ç†å®Œæˆï¼Œå…± {len(df)} ç­†è¨˜éŒ„")
        
        # ARIMA åŸºæº–æ¨¡å‹
        try:
            arima_model = ARIMA(df['Close'], order=(1,1,1)).fit()
            test_size = min(len(df) // 5, 50)
            arima_pred = arima_model.forecast(steps=test_size)
            arima_labels = np.where(arima_pred > df['Close'].iloc[-test_size:], 1, 0)
            actual_labels = df['Label'].iloc[-test_size:]
            baseline_acc = accuracy_score(actual_labels, arima_labels)
            logger.info(f"ARIMA åŸºæº–æº–ç¢ºç‡: {baseline_acc:.3f}")
        except Exception as e:
            logger.warning(f"ARIMA åŸºæº–æ¨¡å‹å¤±æ•—: {e}, ä½¿ç”¨éš¨æ©ŸåŸºæº– 0.5")
            baseline_acc = 0.5
        
        # æº–å‚™ç‰¹å¾µï¼ˆåŒ…å«æƒ…ç·’ï¼‰
        features = ['SMA_5', 'SMA_20', 'RSI', 'Volume_Ratio', 'Sentiment']
        X = df[features]
        y = df['Label']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False
        )
        
        strategy_results = {}
        best_win_rate = 0
        best_strategy = None
        best_params = None
        
        for strategy in strategies:
            name = strategy['name']
            params = strategy['params']
            weight = strategy['weight']
            
            logger.info(f"è©•ä¼°ç­–ç•¥: {name}")
            
            if name == "technical_analysis":
                signals = evaluate_technical_strategy(df, params)
                acc = accuracy_score(y[-len(signals):], signals)
            
            elif name == "ml_random_forest":
                model = RandomForestClassifier(**params)
                model.fit(X_train, y_train)
                preds = model.predict(X_test)
                acc = accuracy_score(y_test, preds)
            
            elif name == "lstm_deep_learning":
                acc = evaluate_lstm_strategy(df, params)
            
            elif name == "sentiment_analysis":
                acc = evaluate_sentiment_strategy(df, params)
            
            else:
                logger.warning(f"æœªçŸ¥ç­–ç•¥: {name}, è·³é")
                continue
            
            performance = f"æº–ç¢ºç‡: {acc:.3f}"
            optimized_params = optimize_params(name, params, performance)
            optimized_acc = acc * 1.05 if acc > 0.5 else acc
            
            strategy_results[name] = {
                'original_accuracy': acc,
                'optimized_accuracy': optimized_acc,
                'optimized_params': optimized_params,
                'weight': weight
            }
            
            if optimized_acc > best_win_rate:
                best_win_rate = optimized_acc
                best_strategy = name
                best_params = optimized_params
        
        if best_win_rate > baseline_acc:
            output = {
                "timestamp": get_taiwan_time().isoformat(),
                "symbol": focus_symbol,
                "best_strategy": best_strategy,
                "optimized_params": best_params,
                "win_rate": best_win_rate,
                "baseline_accuracy": baseline_acc,
                "improvement": best_win_rate - baseline_acc,
                "all_strategies": strategy_results
            }
            
            output_dir = project_root / "data"
            output_dir.mkdir(exist_ok=True)
            json_file = output_dir / f"strategy_best_{clean_symbol}.json"
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(output, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ç­–ç•¥çµæœå·²ä¿å­˜: {json_file}")
            
            message = (
                f"ğŸ¯ {focus_symbol} æœ€ä½³ç­–ç•¥æ›´æ–°\n"
                f"ç­–ç•¥: {best_strategy}\n"
                f"å‹ç‡: {best_win_rate:.1%}\n"
                f"åŸºæº–ç·š: {baseline_acc:.1%}\n"
                f"æå‡: +{(best_win_rate - baseline_acc):.1%}"
            )
            slack_alert(message)
            
        else:
            logger.warning(f"æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„ç­–ç•¥ (æœ€ä½³å‹ç‡: {best_win_rate:.3f})")
            slack_alert(f"âš ï¸ {focus_symbol} ç„¡ç­–ç•¥è¶…è¶ŠåŸºæº–ç·š (æœ€ä½³: {best_win_rate:.1%})")
        
    except Exception as e:
        logger.error(f"ç­–ç•¥ç®¡ç†å™¨åŸ·è¡ŒéŒ¯èª¤: {e}")
        slack_alert(f"âŒ ç­–ç•¥ç®¡ç†å™¨åŸ·è¡Œå¤±æ•—: {str(e)}", urgent=True)

def calculate_rsi(prices, window=14):
    """è¨ˆç®—RSIæŒ‡æ¨™"""
    delta = prices.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

def evaluate_technical_strategy(df, params):
    """è©•ä¼°æŠ€è¡“åˆ†æç­–ç•¥"""
    sma_short = params.get('sma_short', 20)
    sma_long = params.get('sma_long', 50)
    rsi_overbought = params.get('rsi_overbought', 70)
    rsi_oversold = params.get('rsi_oversold', 30)
    
    df['SMA_Short'] = df['Close'].rolling(window=sma_short).mean()
    df['SMA_Long'] = df['Close'].rolling(window=sma_long).mean()
    
    signals = []
    for i in range(len(df)):
        if pd.isna(df['SMA_Short'].iloc[i]) or pd.isna(df['SMA_Long'].iloc[i]):
            signals.append(0)
            continue
            
        ma_signal = 1 if df['SMA_Short'].iloc[i] > df['SMA_Long'].iloc[i] else 0
        rsi_value = df['RSI'].iloc[i] if 'RSI' in df.columns and not pd.isna(df['RSI'].iloc[i]) else 50
        rsi_signal = 1 if rsi_value < rsi_oversold else 0 if rsi_value > rsi_overbought else ma_signal
        final_signal = 1 if (ma_signal + rsi_signal) >= 1 else 0
        signals.append(final_signal)
    
    return np.array(signals)

def evaluate_lstm_strategy(df, params):
    """è©•ä¼°LSTMç­–ç•¥ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
    try:
        sequence_length = params.get('sequence_length', 60)
        df['Price_Change'] = df['Close'].pct_change()
        df['Trend'] = df['Price_Change'].rolling(window=sequence_length//4).mean()
        
        predictions = []
        for i in range(len(df)):
            if i < sequence_length:
                predictions.append(0.5)
                continue
            recent_trend = df['Trend'].iloc[i-10:i].mean()
            pred = 0.6 if recent_trend > 0.001 else 0.4 if not pd.isna(recent_trend) else 0.5
            predictions.append(pred)
        
        binary_preds = [1 if p > 0.5 else 0 for p in predictions[sequence_length:]]
        actual = df['Label'].iloc[sequence_length:].values
        
        if len(binary_preds) > 0 and len(actual) > 0:
            min_length = min(len(binary_preds), len(actual))
            accuracy = accuracy_score(actual[:min_length], binary_preds[:min_length])
        else:
            accuracy = 0.5
            
        return accuracy
        
    except Exception as e:
        logger.warning(f"LSTMç­–ç•¥è©•ä¼°å¤±æ•—: {e}")
        return 0.5

def evaluate_sentiment_strategy(df, params):
    """è©•ä¼°æƒ…ç·’åˆ†æç­–ç•¥"""
    try:
        sentiment_threshold = params.get('sentiment_threshold', 0.1)
        signals = np.where(df['Sentiment'] > sentiment_threshold, 1, 0)
        acc = accuracy_score(df['Label'], signals)
        return acc
        
    except Exception as e:
        logger.warning(f"æƒ…ç·’ç­–ç•¥è©•ä¼°å¤±æ•—: {e}")
        return 0.5

if __name__ == "__main__":
    try:
        mode = None
        if len(sys.argv) > 1:
            if sys.argv[1].lower() in ['us', 'usa', 'american']:
                mode = 'us'
            elif sys.argv[1].lower() in ['tw', 'taiwan']:
                mode = 'tw'
        
        logger.info(f"ç­–ç•¥ç®¡ç†å™¨å•Ÿå‹•ï¼Œæ¨¡å¼: {mode or 'default'}")
        main(mode)
        logger.info("ç­–ç•¥ç®¡ç†å™¨åŸ·è¡Œå®Œæˆ")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ¶ä¸­æ–·åŸ·è¡Œ")
    except Exception as e:
        logger.error(f"ç­–ç•¥ç®¡ç†å™¨åŸ·è¡Œæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
        slack_alert(f"âŒ ç­–ç•¥ç®¡ç†å™¨åš´é‡éŒ¯èª¤: {str(e)}", urgent=True)
        sys.exit(1)
